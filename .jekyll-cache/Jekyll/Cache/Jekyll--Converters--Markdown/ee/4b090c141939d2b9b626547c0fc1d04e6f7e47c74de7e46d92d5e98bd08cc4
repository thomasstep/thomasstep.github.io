I"Û:<p>I recently needed to get CloudWatch Logs to an AWS hosted Elasticsearch cluster via Firehose, and I came across a few sticking points that were not as well documented as I would have hoped. There are quite a few AWS resources involved in getting all of this done. Logs were originally generated by a <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html">Lambda function</a>, which wrote to a <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html">log group in CloudWatch Logs</a>. I put a <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-subscriptionfilter.html">subscription filter</a> on that log group with a <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-destination.html">CloudWatch destination</a> in a different account. That destination forwarded the logs to a <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-kinesisfirehose-deliverystream.html">firehose delivery stream</a> which used a Lambda function <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-kinesisfirehose-deliverystream-processor.html">acting as a transformer</a> before the logs reached their final destination of <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticsearch-domain.html">Elasticsearch</a>. Like I said, quite a few resources. The ‚ÄúLambda function acting as a transformer‚Äù is called a ‚Äúprocessor‚Äù in CloudFormation but it is referred to in <a href="https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html#data-transformation-flow">documentation about data transformation</a>. I will be referring to this Lambda processor as a transformer since that is its main purpose for the intent of this post.</p>

<p>Getting the CloudWatch Subscription Filter and Destination set up were not too difficult, and there is decent documentation <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#DestinationKinesisExample">which you can find here</a>. The one caveat that I will note is that in this tutorial, AWS is building the destination for a Kinesis Data Stream, not a Kinesis Data Firehose (sometimes referred to as a Delivery Stream), so in step 5 where they create the permissions policy with <code class="language-plaintext highlighter-rouge">"Action": "kinesis:PutRecord"</code> change <code class="language-plaintext highlighter-rouge">kinesis</code> to <code class="language-plaintext highlighter-rouge">firehose</code> like this <code class="language-plaintext highlighter-rouge">"Action": "firehose:PutRecord"</code>. (After looking back through that tutorial it appears that they have added <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample">an example for Firehose</a> on the same page. The goal for right now is just to get CloudWatch Logs to Firehose however you would like.)</p>

<p>By the time I got to Firehose, all of the infrastructure had already been set up by someone else. There were still a few remaining problems with the Lambda transformer that I had to stumble through though. Firstly, was the part where I missed how the payload from the transformer should be returned. I originally converted out of Base64, transformed the logs, converted back to Base64, and returned. I thought that Firehose would want the records to be formatted the same way they went in. After scouring Elasticsearch for my logs, I determined that I was wrong. I learned that there is a specific format that all transformed records need to be in. The <a href="https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html#data-transformation-status-model">returned payload needs to send back a flat JSON object</a> that looks like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"recordId"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;recordId from original record&gt;"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"result"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;Ok | Dropped | ProcessingFailed&gt;"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"data"</span><span class="p">:</span><span class="w"> </span><span class="s2">"&lt;base64 transformed record to send&gt;"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>I looked a little deeper into my mistake and how I should format the <code class="language-plaintext highlighter-rouge">data</code> field. CloudWatch Logs sent from a Subscription Filter come in batches but there was only one <code class="language-plaintext highlighter-rouge">data</code> field and one <code class="language-plaintext highlighter-rouge">recordId</code> for each batch. After a record from a CloudWatch Subscription Filter comes in and is Base64 decoded and unzipped it looks like this:</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"owner"</span><span class="p">:</span><span class="w"> </span><span class="s2">"123456789012"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"logGroup"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CloudTrail"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"logStream"</span><span class="p">:</span><span class="w"> </span><span class="s2">"123456789012_CloudTrail_us-east-1"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"subscriptionFilters"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="s2">"Destination"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"messageType"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DATA_MESSAGE"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"logEvents"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"31953106606966983378809025079804211143289615424298221568"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="mi">1432826855000</span><span class="p">,</span><span class="w">
            </span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"{</span><span class="se">\"</span><span class="s2">eventVersion</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">1.03</span><span class="se">\"</span><span class="s2">,</span><span class="se">\"</span><span class="s2">userIdentity</span><span class="se">\"</span><span class="s2">:{</span><span class="se">\"</span><span class="s2">type</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">Root</span><span class="se">\"</span><span class="s2">}"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"31953106606966983378809025079804211143289615424298221569"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="mi">1432826855000</span><span class="p">,</span><span class="w">
            </span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"{</span><span class="se">\"</span><span class="s2">eventVersion</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">1.03</span><span class="se">\"</span><span class="s2">,</span><span class="se">\"</span><span class="s2">userIdentity</span><span class="se">\"</span><span class="s2">:{</span><span class="se">\"</span><span class="s2">type</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">Root</span><span class="se">\"</span><span class="s2">}"</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"31953106606966983378809025079804211143289615424298221570"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"timestamp"</span><span class="p">:</span><span class="w"> </span><span class="mi">1432826855000</span><span class="p">,</span><span class="w">
            </span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"{</span><span class="se">\"</span><span class="s2">eventVersion</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">1.03</span><span class="se">\"</span><span class="s2">,</span><span class="se">\"</span><span class="s2">userIdentity</span><span class="se">\"</span><span class="s2">:{</span><span class="se">\"</span><span class="s2">type</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">Root</span><span class="se">\"</span><span class="s2">}"</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>
<p>That‚Äôs when I found a page mentioning <a href="https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html">combining multiple JSON documents into the same record</a>. This gave me an answer to how I would convert multiple log events into a single Base64 string for the returned <code class="language-plaintext highlighter-rouge">data</code> field. After I successfully rewrote the transformer to output the transformed <code class="language-plaintext highlighter-rouge">logEvents</code> in single-line JSON, I tested it out. This worked whenever <code class="language-plaintext highlighter-rouge">logEvents</code> was only one object long, but it did not work whenever larger batches were passed in. I went back to the drawing board and read something about Elasticseach bulk inserts needing to be newline (<code class="language-plaintext highlighter-rouge">\n</code>) delimited. I added a newline in after every JSON object instead of leaving them as single-line JSON just to test it out. This time I got errors back from Elasticsearch saying <code class="language-plaintext highlighter-rouge">Malformed content, found extra data after parsing: START_OBJECT</code>. At least this was something concrete to go off of.</p>

<p>I started Googling and found a <a href="https://www.reddit.com/r/aws/comments/8yqwh2/cw_logs_to_firehose_to_elasticsearch_multiple/">Reddit thread</a> and <a href="https://forums.aws.amazon.com/thread.jspa?messageID=852983#852983">AWS support ticket</a> that were tied together. It turns out you need to reingest the records individually so ElasticSearch only receives one log at a time, and I found an example in a deep dark rabbit hole. A <a href="https://serverlessrepo.aws.amazon.com/applications/us-east-1/107764952090/kinesis-firehose-cloudwatch-logs-processor">solution is mentioned that can be found through a lot of digging on AWS‚Äôs site</a>, and <a href="https://github.com/tmakota/amazon-kinesis-firehose-cloudwatch-logs-processor/blob/master/index.js">I found the GitHub repo for that code</a>. There is an example of reingestion near the bottom of that application. I ended up using <a href="https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Firehose.html#putRecordBatch-property"><code class="language-plaintext highlighter-rouge">putRecordBatch</code></a> from the AWS SDK to put the <code class="language-plaintext highlighter-rouge">logEvents</code> back into Firehose as individual records.</p>

<p>When I converted over to reingesting <code class="language-plaintext highlighter-rouge">logEvents</code> as individual records, I finally saw logs in Elasticsearch. But now I saw too many logs. Strange. The logs were being duplicated. When I changed <code class="language-plaintext highlighter-rouge">result</code> to <code class="language-plaintext highlighter-rouge">Dropped</code> in the response for the original CloudWatch Logs batched record, I changed <code class="language-plaintext highlighter-rouge">data</code> to a hardcoded string. What a previously mentioned link (https://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html#data-transformation-status-model) fails to mention is that you can‚Äôt have <code class="language-plaintext highlighter-rouge">data</code> if <code class="language-plaintext highlighter-rouge">result</code> is <code class="language-plaintext highlighter-rouge">Dropped</code> or Firehose will reprocess the record. Once I got rid of the hardcoded string in <code class="language-plaintext highlighter-rouge">data</code> in the transformation Lambda‚Äôs response, everything clicked and I started seeing my logs in Elasticsearch without being duplicated.</p>

<p>The code in the aforementioned GitHub repo does something that I wanted to highlight. After Base64 decoding and uncompressing a record, check for <code class="language-plaintext highlighter-rouge">record.messageType === 'DATA_MESSAGE'</code> (Javascript). This condition signifies that the record is coming straight from CloudWatch Logs but a record without <code class="language-plaintext highlighter-rouge">messageType</code> means that the record is being reingested. Making this check also allows Firehose log producers to directly PUT to Firehose as we do in the transformer for reingestion. I would suggest making all of your transformations on the reingested logs only and do not make any logic along the lines of checking how many <code class="language-plaintext highlighter-rouge">logEvents</code> are coming from CloudWatch Logs. Just automatically reingest and handle the transformation on the reingested logs, it will make the logic much easier to debug.</p>

<p>There were numerous errors that I had to debug while getting this setup, and the answers were vague and difficult to find. I hope this can be your last stop if you are running into the same issues I was.</p>
:ET