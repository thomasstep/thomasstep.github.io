---
layout: post
title:  "Regular Expression Vulnerability"
author: Thomas
tags: [ dev, security ]
description: TBD
---

I stumbled across something interesting the other day that I dove into and wanted to share. There exists a vulnerability called [ReDos  (Regular expression Denial of Service)](https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS) which results from a poorly written regular expression taking a long time to complete matching. If an attacker can determine that a regex has been written with this type of vulnerability, they can exploit the long running times to block processes. Since regexes are used for loads of different web services nowadays, I think it is important to understand the time complexities behind regex algorithms and the unintended consequences they could have. I now know that I have written some regexes in the past that are vulnerable, and it is easy to introduce these into code.

One specific variant of ReDos that I came across was [catastrophic backtracking](https://www.regular-expressions.info/catastrophic.html). The linked article has more information in it, but the high-level idea is that some regex algorithms that use backtracking to find matches can be given strings that result in exponential time complexity. Anyone who has had a class on discrete math or written an inefficient algorithm, either intentionally or unintentionally, will know that exponential time complexities have a noticeable effect on user experience. That noticeable effect is how attackers can exploit this vulnerability.

Nested quantifiers are at the root of this problem. Something along the lines of `(b+)+z`. The `+` nested inside of a group that is followed by a `+` sparks the potential for an exponential amount of backtracking attempts. Given a string with repeated `b`s in it and a `z` at the end, this regex string would first be greedy and attempt to find a match after matching all of the repeated `b`s for the inner nested `b+` before it found the final `z`. At that point, the algorithm would declare a match and end. If there is no `z` at the end of the string, we run into our problem. The algorithm will give up the last `b` for the inner nested `b+` and attempt the match again. The algorithm is backtracking out of its greediness in hopes that a match can be found. If that still does not work, the algorithm backtracks again and gives up 2 `b`s. This continues on and on until all of the options are exhausted. This is how the exponential time complexity comes into play.

![Screenshot of Pomodoro Noise in the Apple App Store](/assets/img/redos-example.png)

The screenshot above is from regex101.com’s debugger showing what happens when we apply this regex to a string of only `b`s. No match was found and we can see that there were 6130 steps needed to determine this. To put this into perspective, this regex takes 6 steps to complete and find a match using `bbbbbbbbbbz` (the same string with a `z` on the end). Starting from a string consisting of one `b` and going to ten `b`s (what is featured in the screenshot above) takes 7, 18, 41, 88, 183, 374, 757, 1524, 3059, 6130 steps, respectively. We can already start to see the exponential increase just from `n = 1` to `n = 10`.

To help mitigate this, one quick and simple method is to apply atomic groups to the nested repetition: `(?>b+)+z`. Testing this from one to ten `b`s looks like 7, 12, 17, 22, 27, 32, 37, 42, 47, 52 steps, respectively. These results show a linear time complexity, which is much better. The atomic grouping (`?>`) is explicitly telling the engine not to backtrack. We can do this because we know there is no way it could find a `z` if the string just matched all `b`s. This is simply one example of a problematic regex and a solution. Others need to be considered as well, and I suggest reading through [this page](https://www.regular-expressions.info/catastrophic.html) and [this page](https://www.regular-expressions.info/redos.html) to learn more about them.

This is all fine and dandy, but how does it apply to me? Well, have you ever told a regex to “match anything”? The notorious `.*`. I bet most people who have written a regex know what that is and have used it. I know I have. The problem with `.*` combined with the greediness of most regex engines is that it will most likely swallow up the entire string before backtracking to match with the rest of the regex. If a `.*`, or any other quantifier, somehow slips in next to another quantifier, a vulnerability might have been introduced. [Backtracking issues can also stem from](https://www.regular-expressions.info/redos.html) nested quantifiers or `OR`s that can match the same text. Some patterns unveil potentially problematic regexes, but I’m going to keep this vulnerability on my radar anytime I need to write a regex.

After hearing about this, I would imagine that it’s fairly uncommon; however, this has happened to [Cloudflare](https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/) and [StackOverflow](https://stackstatus.net/post/147710624694/outage-postmortem-july-20-2016) fairly recently. Problems seem to come from regexes that are added to or created without thinking about cases that don’t match. It can be difficult to make them match in the first place, so developers don’t have time to think about what could go wrong after implementing them. Also, the cryptic-looking nature of regexes doesn’t invite debugging or the desire to walk through the algorithms that live behind the scenes. On top of all that, I hadn’t even heard of this before a few days ago when I stumbled across mentions of it. After reading up on ReDos, my new approach while creating regexes will be to use a debugging program like regex101.com where I can walk through the steps needed to match a string and see what happens if that string doesn’t match. I have not seen too many posts that explicitly highlight this vulnerability, but it is only fair that I mention [this article](https://levelup.gitconnected.com/the-regular-expression-denial-of-service-redos-cheat-sheet-a78d0ed7d865) that also helped me understand what was going on behind the scenes.
