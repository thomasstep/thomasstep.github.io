## Introduction

I have worked extensively with AWS for multiple years. I have run production applications on AWS and I have personally created and built products and multiple one-off APIs using AWS. I have worked with more traditional architectures, but after learning about serverless architectures, I do not think I can ever go back. My goal with this guide is to give my stance on how to build applications with AWS by discussing my favorite services and show how to tie them all together.

Anyone from a traditional architecture background may find that this is "buzzwordy" to which I say, "fair." However, there is a reason that building software with serverless is popular: it involves less upfront work, and operation maintenance is reduced. Delivery speed is the name of the game in software. Being able to spin up a completely new API within the span of a few minutes should be an argument enough. Creating that same API with built-in autoscaling, efficiency, and security is icing on the cake. Let the people at AWS handle operations so we can focus on writing software for our specific use cases.

I am a software engineer that happened to learn DevOps and cloud engineering. I am not claiming to be the end-all-be-all resource on cloud or AWS knowledge; however, I am one of the few people I have met that have an understanding and interest of both the software and operational sides of build applications. For that reason, putting together this guide seemed valuable for both myself and others. I run in two very different circles on the internet and I hope to bridge that gap.

Operations and development seem to not want to cross-pollinate despite the emergence of "DevOps" and despite everyone calling themselves a DevOps engineer. When I hear DevOps, my mind goes to someone like myself who can develop the application code and also write infrastructure as code to host and run that application code. From what I have gathered online, this is not the norm. Instead, most people take DevOps to mean someone who used to be purely in operations, learned how to script in Python, and now does mostly the same operations work but with a cloud provider. On the other side, very few of the software engineers I know have had any sense of wanting to learn more about the infrastructure side of things. It is a true shame because writing software and provisioning infrastructure for the application go hand in hand.

Writing code for a serverless API is not a traditional method of writing code. I can only imagine how long it will take for colleges and boot camps to even realize that enterprise-grade code does not run on a virtual machine in a company-maintained data center anymore. There's a definite knowledge gap that I hope to start filling with this guide.

To achieve my goal with this guide, I will introduce some operational concepts and services available through AWS that implement those concepts, then I will give some ways that I have successfully integrated them. I do not plan to give highly technical examples or API calls because AWS could change those at any time. I want to instead give insight into how these services work at a higher level so that the tools available to us can be better understood instead of giving copy and paste examples. AWS is just a big set of building blocks, and how we put them together is where we can create value. Knowing what is available and how to lay down a foundation is the starting point into the vast world that is AWS.

The services I write about are foundational services that I believe new learners should focus on. I hope to write this guide the same way I would speak to a colleague while they are first starting their journey. I want to expose you to key services and what they can do, but I do not intend to create a one-stop-shop for learning everything you need to know about AWS. As AWS is constantly changing, learning the fundamentals is more important than learning the implementation details or provisioning process. Also, once you know the provisioning process and integrations for one service, provisioning and integrating other services becomes vastly easier.


## Serverless Introduction

I almost exclusively use serverless technologies. Some of the more popular AWS serverless offerings I use include Lambda, API Gateway, SNS, and DynamoDB. Each of these has its purpose that I will go into throughout this guide. I want to keep a focused lens for this guide and not let in scope creep.

I have multiple products lingering around the internet that are all built on serverless technologies. The hosting costs are zero until someone pays me first to use a service. From what I have seen from others and experienced in my professional life when it comes to AWS costs I am unique in the fact that I do not pay anything until my products get used. Cost is one of the largest benefits of using serverless architectures and one of the largest differentiators that I look at before judging a service as one that I want to use. It may be argued that serverless technologies cost more down the road while under load or with consistent traffic, but I have seen that the opposite is true as long as the code is written to go hand-in-hand with the architecture.

Before getting into AWS and operations, I was a software engineer. I am still a software engineer, and I thoroughly enjoy writing code. The main reason I got so deep into AWS was that after I wrote the code, I needed some way to share it. Anyone can spin up a server on their local machine and make requests to `localhost`. An added layer of complexity arrives when it comes time to host that code or service publicly. AWS and the opportunity for abstraction it provides, allows me to handle that complexity. That being said, I like to understand what goes on behind the scenes of any technology that I work with, so I will offer the same explanation in this guide. However, using the serverless services that AWS provides allows me to focus on writing code while managing infrastructure becomes the easy part. This is another huge benefit of serverless: time savings. Not only does setting up a serverless architecture take less time than traditional infrastructure, but it also extends the time savings to generic boilerplate server code. I don't write routing logic anymore, I just create a new Lambda function with the logic and tell API Gateway to handle the rest for me.

Serverless provides advanced infrastructure benefits that used to be difficult to orchestrate like autoscaling and computing efficiency. I have seen many guides online about autoscaling for containers and EC2 instances. With a service like Lambda, load balancers and scaling alarms are a non-factor. All of that is handled out of the box. Compute resources and therefore cost is also optimized. Using a Lambda function means that I have only ever paid for the underlying compute resources when I use them. Compare that to always paying for an EC2 instance regardless of the utilization and the cost-benefit becomes clear. The best part is that there are probably benefits and optimizations working behind the scenes and constantly being updated that I do not know about. Using serverless technologies means that developers can develop while a dedicated operations team at AWS handles the infrastructure.


## Introduction to the Cloud and AWS

Boiled down to its core, cloud providers are simply offering computers and storage to anybody wanting to use their resources. Some companies that are large enough to benefit from economy of scale will own and manage their own data centers, but using a cloud provider mostly likely makes more financial sense before reaching that scale (and sometimes even afterward).
For those of us who are smaller players, using a cloud provider fits the bill well. There is a low overhead for us because we do not need to work out the finer data center details or struggle with the overhead of purchasing servers. Also, there are performance benefits both in computation and reliability. Purchasing mass volumes of high-end servers for a smaller player makes little to no sense when compared to running the same service on a cloud provider's managed servers. Furthermore, purchasing enough servers and appropriately networking them to have failover and geographic redundancy would skyrocket the cost. At a scale like Amazon, Google, or Microsoft, it makes sense to have multiple data centers spread out all over the world with racks upon racks of servers and disks. For a bootstrapped startup, buying a single server to host an app is almost out of the question not to mention the operational overhead of getting it up and running.

AWS is simply a cloud provider. The concepts used for building, deploying, and running applications on AWS can be abstracted out to any cloud provider. My goal is not to sell anyone specifically on using AWS, but I believe AWS currently has the best service offerings and has the largest community around it. Specific serverless services make arguing against AWS difficult especially with the cult-like following of some including Lambda and DynamoDB. On top of that, AWS makes DevOps available to even the smallest players with serverless CI/CD tools like CodeBuild, CodePipeline, and CodeDeploy.

There are plenty of PaaS (Platform as a Service) out there, but using AWS and having full control over your infrastructure comes with great benefit. Configurations can be tweaked and optimized, new services can easily be integrated, and it is plain fun to build out infrastructure. Having more granular control also means more granular pricing.

By using AWS and reading through this guide, I hope that I can offer a framework for quickly building applications. I believe that by harnessing all of the hard work that AWS (or any cloud provider for that matter) does with regards to deploying and hosting code, building an application is easy than ever before in history. Speed is the name of the game, and using AWS means getting stuff out quicker.

Before we dive much deeper into understanding specific services, we need to touch on two other services that are foundational to everything in the cloud: Infrastructure as Code (IaC) and IAM (Identity and Access Management). Both topics serve an enormously important role in the cloud, and I always recommend learning the concepts behind them before jumping into other services. I learn best by doing things, so in the cloud learning arena, I learn best by provisioning infrastructure and using it. Since IaC and IAM are tools that touch every other service, I do not think that practicing only IaC or IAM is completely necessary because practicing using these two areas will happen in parallel with everything else.


## Infrastructure as Code

Infrastructure as Code enables repeatable infrastructure provisioning and configuration. This concept is what allows us to move quickly and abstract out ideas and patterns that we use often. The AWS-native method of writing infrastructure as code is using a service called CloudFormation.

CloudFormation ingests our requests in the form of "templates" written in YAML or JSON (I suggest using YAML) and provisions infrastructure according to what the template specifies in something called a CloudFormation Stack. The CloudFormation Stack can then be updated or deleted and multiple stacks can be created based on the same template. I like to think of CloudFormation templates as a declarative way of calling AWS APIs to provision infrastructure with certain configuration settings. A similar idea would be writing a program without variables and hard coding every value. That being said, there are ways to make CloudFormation templates "dynamic" by substituting in environment-specific values using parameters, pseudo parameters, AWS Secrets Manager, and AWS Systems Manager Parameter Store.

Learning CloudFormation is a good first step into learning IaC, but after thousands of lines of YAML, it is almost inevitable that patterns will start to emerge that would be better abstracted out. There are ways to do this natively in CloudFormation by using something called CloudFormation Nested Stacks; however, I do not recommend using them. I have heard horror stories from trusted resources when it comes to maintaining and updating those Nested Stacks. My suggestion is to either bite the bullet and copy-paste those walls of YAML text or look into IaC that leverages a programming language to build the infrastructure.

Spend some time hanging around cloud and DevOps forums and in no time at all there will be mentions of third-party IaC providers like Serverless Framework, Hashicorp Terraform, or Pulumi, to name a few. Some are kept more updated than others with Terraform seeming to be the prevalent provider. My suggestion is to stick with first-party solutions though. The latest addition to this menagerie of IaC using a familiar programming language is AWS's own called the CDK or Cloud Development Kit.

Since learning and experimenting with the CDK, I have started migrating my IaC to CDK applications. The CDK is a way to define infrastructure using a programming language instead of YAML templates. When a CDK application is "synthesized" it outputs CloudFormation in YAML or JSON, so the CDK is effectively a layer on top of CloudFormation that we can interact with using programming languages that output static templates.

Keep in mind that I am a software engineer at heart. When I learned that I could mesh my cloud knowledge with programming, I was excited. Abstracting out ideas and making them repeatable is a core concept in programming, so the ability to use what I knew already from that arena and apply it to cloud resource definitions would make my life much easier and more succinct.

I see loads of beginner AWS tutorials that start out using the AWS console. While I also get some amount of benefit from visual verification, do your best to start using IaC as quickly as possible. The benefits are too large to become dependent on using the console. Two of those benefits that I lean on heavily are replicability and programmatic deployments.

Using CloudFormation templates means that we can deploy a stack knowing exactly what resources will be provisioned on our behalf. Creating another version of an application becomes a matter of deploying a new CloudFormation stack using the same template(s) as before. That replicability helps me sleep well at night. If a resource fails or someone makes a breaking manual change (things that happen all the time), all we need to do is recreate or update our CloudFormation stack to get everything functioning as intended again.

Using CloudFormation templates and storing those alongside the code which runs on the infrastructure in source control means that we can enable DevOps processes. This goes hand-in-hand with replicability. We can create a CloudFormation template once and deploy it multiple times to produce the same application infrastructure. CodeBuild, CodeDeploy, and CodePipeline are all developer tools offered by AWS that help with deploying and updating CloudFormation stacks based on external triggers like a push to GitHub or a pull request being merged.

In the following sections highlighting specific services, I encourage experimenting with them solely through IaC and CloudFormation. All of the services that I use and will bring up in this guide are integrated with CloudFormation. The AWS documentation is good in this area (not so much for all of their services) so a quick search including "cloudformation" plus the service you are attempting to provision should bring up the corresponding documentation.

## IAM

IAM stands for Identity and Access Management. This service and concept can be annoying but it is, unfortunately, part of everything. IAM is one of the few fundamental services that AWS offers and it is related to security. IAM consists of a few important resources which include Users, Roles, and Policies. All of these IAM resources work together for AWS resources to tell other AWS resources what they are authorized to do similar to a traditional AAA (Authentication, Authorization, Accounting) service. For example, a Lambda function with an IAM Role attached to it that allows access to a specific DynamoDB Table named "lambdaTable" will only be able to access that table. The same Lambda will get an Access Denied Error if it tries to access a DynamoDB Table named "fargateTable."

IAM Users can be thought of as standalone AWS resources that represent a certain person or application that interacts with AWS. For example, I could create an IAM User for myself to have credentials for interaction through the command line, and I could create an IAM User for a third-party CI/CD service that needs access to deploy to AWS on my behalf. Users can have static credentials called Access Keys with which AWS can be accessed through the command line. The configuration process can be done through environment variables or static files, but I suggest searching for the current AWS documentation on that process instead of me giving specifics.

IAM Roles are a logical grouping of Policies. IAM Roles in and of themselves do not do anything. A Role needs to be assumed by another resource (an IAM User, Lambda function, etc.) before it becomes useful. The resource that assumes the Role then receives all of the permissions defined in the Policies contained in the Role.

IAM Policies are the fundamental element of authorization in AWS. Policies are JSON documents that define what actions resources are allowed to perform. There are two types of Policies: identity-based Policies and resource-based Policies.

Identity-based Policies can be built as standalone resources or attached directly to IAM Users and Roles. The resource that assumes a Role or a User with an attached Policy will only be able to operate within the bounds of what the identity-based Policy defines as allowed.

Resource-based Policies are attached directly to non-IAM AWS services. The most common service that can have Resource-based Policies attached to it is S3. The Resource-based Policy would define what other resources could act upon the attached resource and what operations the other resources could perform. Resource-based Policies allow us to create restrictions or allowances on AWS resources regardless of the permissions defined in the Identity-based Policies that they are associated with since Resource-based Policies' permissions boundaries take precedence over Identity-based ones.

Users, Roles, and Policies lightly play with each other but do not necessarily always need to. Policies are the fundamental portion as we have already seen. A Policy can function as a stand-alone resource that is associated or "attached" to multiple Users, Roles, or Resources, which are called Managed Policies. Policies can also be written as Inline Policies in which case they are one-off and only attached to a single User, Role, or Resource. Roles are bundles of Policies both managed and inline. Roles can be assumed by Users or other AWS resources during execution. Users are made up of managed and inline Policies, but Users can also "assume" Roles. It can be complicated, but after experimentation, IAM becomes easier to understand. I will share the most common pattern that I have used in the past to manage the various IAM resource types.

I normally start by creating a few Policies with a given set of permissions, for example, one Policy for reading from DynamoDB, one for reading from Kinesis and writing to S3, and so on. Those select few Policies become a standard issued set which can then be attached to various Users and Roles. Users are created for particular people or services and are somewhat one-off. The same idea with Policies would then apply to Roles, so a select few Roles are created with semi-broad but still restricted permissions defined in the Policies. Those few Roles are then referenced by various AWS resources to be assumed during execution. In this way, only the select few Policies and Roles need to be maintained reducing operational overhead for this area of AWS. The shortcoming is that the Policies are slightly broader than they may need to be which is not a security best practice, but maintaining Policies and Roles for every service can be cumbersome.

IAM is important but I do not want to spend too much more time discussing them. Much of IAM can be learned in a trial-by-fire process. AWS does not always have the best error messages, but figuring out that a service is missing permissions to access a certain resource is normally easy. Determining which permissions to add to an IAM Role or Policy is also fairly straightforward after seeing one of those errors. One way to go about doing this (although not best practice) is to provide a service with blanket permissions from the get-go. Later, those permissions can be narrowed down to specific CRUD (Create, Read, Update, Delete) permissions or specific API calls even.

All that being said I currently rely on IAM Roles generated by the CDK or I use IAM Roles that I have created multiple times over in CloudFormation in the manner that I discussed earlier. Just like anything else in computing, patterns will emerge for permissions that certain services need and those can be made into generic templates. The CDK handles IAM automatically and really well. Most of the time, IAM Roles do not even need to be defined through the CDK, and adding the required permissions for a given service becomes as simple as an API call. I highly suggest learning the ins and outs of IAM, but CDK puts all of that on autopilot once you understand what is happening behind the curtains.

## VPC

Virtual Private Cloud (VPC) is a fundamental AWS service so much so that every AWS account comes with a VPC by default. However, I do not want to spend much time discussing VPC. If you only ever use the services that I outline in this guide you do not necessarily need to worry about creating a new VPC or even using the default VPC, but you will surely read about them after diving into the AWS realm.

For anyone coming from a traditional networking or operations background, a VPC is pretty much a VLAN. For anyone who does not know what a VLAN is, think about a VPC as a way to completely separate your AWS resources from other resources. Resources in two different VPCs using respective private IP addresses will not be able to talk to each other unless certain non-default conditions are met. This grants more security and helps teams divide up resources based on workload or environment. The downside of learning about VPCs is that AWS assumes everyone has existing knowledge about networking.

Deploying certain resources into a VPC is almost a given, but none of those resources are talked about in-depth in this guide. Some popular VPC-dependent resources include EC2 (which are self-managed virtual machines), ECS Fargate (serverless containers), and RDS (relational databases). I would not worry about VPCs to get started with because it is probably too deep of a rabbit hole to go down into unless you know for sure that your specific workload will need one.

## Lambda

AWS Lambda is the pinnacle of modern-day serverless. Some people even associate the term serverless directly with Lambda functions. However you choose to think about Lambda, there is no denying that it brought about a new paradigm in computing.

There is no real traditional alternative to Lambda. The closest idea to a Lambda function in the traditional sense would be a Virtual Machine (VM) that spins up whenever a request comes in then scales down to zero after a period of inactivity. AWS Lambda offers Functions as a Service (FaaS), which means that all we as developers need to do is tell Lambda what language our code is written in and it handles the rest. Leveraging Lambda is by far the quickest way to get code up and running in the cloud. Code can be delivered to Lambda in the form of a zip file or a container, but my personal preference is to use a zip file. The best part about using Lambda is the built-in administrative features that are jam-packed into the service like auto-scaling, fault tolerance, high availability, efficiency, and flexible pricing.

### High-Performance Computing Features

Lambda scales on its own and as needed. It will not take long after diving into the world of serverless before encountering the concept of cold starts though.

A cold start is an overhead time that AWS (or any other cloud provider for that matter) requires to create the necessary resources to support a new instance of a Lambda function like spinning up compute resources and attaching network interfaces. AWS constantly improves their end of the cold start problem, but the other half is up to us. Keeping the deployment package size down, properly managing dependencies, and understanding which pieces of our code run at different points in a Lambda's lifecycle will all help reduce the amount of cold start time.

After nailing down the infamous cold start dilemma, nothing is holding Lambda back from handling massive amounts of traffic. It is worth noting here that code for Lambda functions needs to be written without requiring state and with horizontal scaling in mind. The key here is quick, lightweight, and idempotent code.

Reaching into high-scale territory comes with inevitable hardware failures. Errors will unveil themselves at the worst times, but Lambda functions are primed to handle that uncertainty. Idempotent code is a key consideration here. If clients are prepared to retry calls to your Lambda function, then a stray error will seem like a blip on the radar. If something does go wrong with the underlying infrastructure of a Lambda function, it will result in a new instance spinning up just like any other compute resource would while scaling up. We do not have to worry about tracking down failing hardware, that is Amazon's job and they do it quite well.

Code should be written and infrastructure should be configured with fault tolerance in mind. For both code and infrastructure, failed attempts should be retried after a given time. Any number of issues could spring up between resources; there could be networking, hardware, or application code issues just to name a few. If an expected response is not received within a given amount of time, a client should assume there was a fault somewhere along the way and retry.

One example of this that is easy to get working is the integration between Lambda and AWS Simple Queueing Service (SQS). Lambda polls SQS then processes the event. If the Lambda function times out or returns an error, then SQS keeps the event in the queue allowing another Lambda function to attempt processing it. After a preconfigured amount of processing attempts, SQS finally either rejects the event as a bad event or forwards the event to a Dead Letter Queue (DLQ) to be handled by some other manual or automated process.

High availability is an area that traditionally takes massive amounts of resources to implement because it involves geographically separated data centers and correctly configured networking equipment and servers. For small businesses and applications creating true highly available services is almost out of the question because it involves a lot of operational overhead. Luckily for us, AWS already took the liberty of setting up the necessary operational aspects and made the remaining configuration easy for us.

The operational overhead required is evident for all AWS services not just Lambda. AWS owns numerous data centers in numerous geographic regions all over the world. Regions in AWS are specific states or cities around the world that are clusters of data centers also known as Availability Zones (AZs). Availability Zones are discrete data centers within a Region and there are multiple Availability Zones in each Region. We can provision the same Lambda function (or sets of infrastructure for that matter) in various Regions or Availability Zones to implement geographic redundancy and promote high availability.

AWS services make it easy to provision duplicate sets of infrastructure or whole applications in multiple Regions, and specific services like Route 53 (AWS's DNS service) make it easy to fail over to working Regions if a certain data center or set of data centers become disconnected from the network.

While high availability might not be a concern for smaller businesses and applications, this becomes a definite concern as applications serve more and more users in various regions of the world.

Efficiency is near synonymous with Lambda. Just the idea of a serverless function promotes efficiency. There is no constantly provisioned hardware so other cloud users can share the same underlying resources. For our side of the equation, we need to keep code small and clean to make sure that the function runs quickly. Developers who know how to properly manage Lambda functions are also able to efficiently prototype and create new services as needed.

### Pricing

Pricing goes hand-in-hand with efficiency, especially from a hardware standpoint. Without provisioned servers, we only pay for server time while Lambda is processing a request. For me, this is one of the hugest benefits of using Lambda. I have created entire applications and never paid for servers. There is also a free tier which means even when my applications are being used, I only pay after a certain point. My life remains stress-free because I do not need to worry about paying for a server especially when it is not being used.

Development time costs money. Ask any software engineering manager and they will tell you the same thing. Quality software engineers command a high salary, so whatever time they spend working on a project should be optimized. Developers spending less time fiddling with infrastructure and more time working on core business logic means a lower overall price for an application to be built. Even when I work on my side projects, I want to get straight to coding to see my idea come to life as quickly as possible. Using Lambda facilitates quick development and deployment cycles.

### Developing With Lambda

Speaking of development, developing for Lambda can be a bit tricky at times. At a high level, we need to give the Lambda function code that accepts two main arguments: `event` and `context` and configure the Lambda function with the correct file path to reach said function. Writing a synchronous handler in Node.js (I like to use Javascript where I can to make context switching a little easier) means that the Lambda function will be passed a third parameter which is the callback function. The `event` contains relevant information from the calling service or request. Each AWS service has a different calling signature or schema, which is best referenced in the AWS documentation. The `context` argument contains information relating to the Lambda function's configuration.

Writing Node.js functions changes the way that the Lambda function returns the result of its execution depending on whether or not it uses `async`/`await` or `Promise` style syntax. I prefer using `async`/`await` which requires the handler function to either return its result or throw an error. The alternative is to use the callback function provided as the third argument to the Lambda handler. There are some caveats worth mentioning when developing for Lambda and some specific to Node.js.

Firstly, Node.js uses an event loop so running async functions can lead to unintended execution. This was one of the largest surprises to me when learning to develop with Lambda. One specific example I have was trying to call an API in the background to notify it of an event. I did not use nor care about the result of the API call, so I did not `await` the result and returned the handler function. I starting noticing weird behavior though. Despite the Lambda function completing every execution, the API call was not always executed.

The problem stemmed from the way that Lambda operates because it assumes that after the handler function returns, it can shut down the computing resource backing it. This means that if the API call was not sent before the handler function returns, the server that ran the code might have already shut down.

Another facet of developing with Lambda is that the Lambda function might continue to run on subsequent requests with the same "execution environment" which more or less means the same server or compute resource backing it. Reused execution environments might allow the previous event loop to continue, which would complete the asynchronous function execution as if the server had never shut down. This resulted in difficult tracings because I would see an API arrive multiple seconds after the originating Lambda (which had been shut down and now rebooted) had initiated the call.

Reused execution environments are not always bad though. Despite their sporadic nature, there is a possibility for saving time depending on how the Lambda function's code is structured. The same way a reused execution environment will continue to process the old request's event loop, all of the globally defined variables will be available in the reused environment. If there is expensive code that needs to be run at the beginning of every request, there is an option to run that code in a global scope and reuse it across contexts or requests. It is a little finicky, but it is called out by AWS as a best practice for reducing latency for end-users.

The way to reuse variables across execution environments is by declaring them at the global scope, which is a great low-cost way to cache those variables. Of course, the locally cached values are not guaranteed to be present, so we need to build in a check before assuming the locally cached values are present and defining the values for those variables if they were not carried over from the last execution. In a way, we need to initialize the environment if it had not previously been initialized.

A bit of a disclaimer around all of this before I go any further. I have previously mentioned that Lambda functions are stateless and should be thought of as stateless environments. While we can cache values in memory, those values should not be associated with the state of an application since those values could vanish at any given time. For a more durable caching solution, I suggest using DynamoDB (although I am sure that will receive bad reviews since DynamoDB is not particularly a cache, please read through the DynamoDB section for more) or a more traditional cache like Redis.

Shifting back to the execution environment topic, we can write code to help initialize a Lambda's environment and not pay for the time spent initializing. The downside to that is increased cold starts. What I am referring to is writing code before a Lambda's handler function is defined. This allows us to perform common tasks like checking for existing cached values from a previous execution environment, retrieving necessary secrets, and creating database connections. Nothing is without its drawbacks, so while we are advantaged by being able to initialize our environment without paying for it, we take a hit in cold start times which reflect in higher latency for users.

On the topic of cold starts, there is a fair amount of work that developers can do to ensure quicker cold starts. We have already explored one way which is to keep execution environment initialization time down which is a direct one-to-one in terms of increasing cold start times. Another bit of work is properly managing the size of a Lambda's deployment package.

I have seen a bit of back and forth online about the impact of deployment package size on cold starts, but from my personal experience, there is an effect. I try to keep my package sizes to a minimum.

My first suggestion to help with deployment package size is to make sure that only directly related code and files make their way into the deployment package. I once encountered a scenario that involved an entire codebase being deployed to a Lambda function including unrelated source code, tests, and CloudFormation templates. Tuning the Continuous Integration scripts to only package the appropriate source code decreased the overall size by half.

Another area that I keep my eyes on is the added size of code dependencies. I prefer writing code in Javascript, which has a terrible reputation for dependency bloat. Regardless of the language used, check the size of a dependency on the package management site like `npm` or `pip` before adding it because a single dependency might double the size of the final deployment package.

My latest area of improvement for decreasing cold start times is properly architecting and organizing Lambda functions. Lambda is a quick and agile platform, which means it is easy to simply create a new Lambda function instead of continually adding complexity to an existing one. A synchronously invoked Lambda means that a client experiences the execution length of that Lambda in the form of latency. Make sure that all latency is required latency, and make sure that all code being run is directly necessary for the returned value. If any code is unused, move it to another Lambda function and invoke that Lambda asynchronously through another service like AWS Simple Notification Service (SNS) (there is a chapter on SNS later in the guide).

When it comes to provisioning and deploying Lambda functions, the options to do so are endless. It is possible to package a Lambda function as a container image now; however, my personal preference is still to use zip files. Zipping files is simple and universally recognized, which means CI/CD and package storage is easily integrated with other AWS services as well as third-party services. One integration that I have successfully deployed multiple times over is using an AWS service called CodeBuild for Continuous Integration, AWS S3 for artifact storage, and AWS CodePipeline and CloudFormation for Continuous Deployment. (More on these services in later chapters.) However, for smaller projects, I believe that locally run scripts are perfectly capable of handling the job as well as larger, more orchestrated integrations.

Running integrations between AWS's developer tools in the cloud is almost required in more elaborate and enterprise settings. CI/CD can be triggered by webhooks on pushes to GitHub or periodically and there is no human error when that process is automated. For smaller players or side projects, those complex integrations can be expensive in terms of development overhead and also the cost of the services themselves. It is important to note that just about every action for every service that AWS offers can be triggered through an API call. The AWS CLI exposes those API calls and is an incredibly useful tool not only for CI/CD tools but for every service.

### Integrations

There are a plethora of integrations between Lambda and other AWS services. I like to think of Lambda as the glue between AWS services. Any custom logic that needs to be injected into any series of events or flow of data will most likely be handled by a Lambda function. Stopping to think about it, it makes total sense. A Lambda function is a light and nimble platform that can be invoked by anything either synchronously or asynchronously. I wanted to bring up and highlight a few integrations between AWS services that I have deployed in the past to give an idea of what is possible.

The first integration is one of my favorites and a tried-and-true one that almost everyone knows about or has implemented. Lambda and API Gateway has got to be the simplest way to deploy an API today. There is no server set up only coding the business logic that lives inside of the Lambda. Since I have a section dedicated to API Gateway I will hold off on more discussion until then.

Engineers sometimes have trouble shifting to a serverless mindset because of the idea of ephemeral infrastructure. How are we supposed to execute a long-running job that we know would extend past Lambda's 15-minute timeout? The answer is AWS Step Functions. While there are other integrations besides Lambda functions, Step Functions allow us to orchestrate the execution of multiple Lambda functions. While this might not directly solve the issue of a single long-running job, it does allow us to break that job into multiple pieces or steps, create individual Lambda functions based on those steps, and orchestrate those Lambda functions. Deconstructing a long-running application like this is good practice and allows for fault tolerance and a better understanding of what is truly happening.

Cron jobs are commonly used in traditional operations settings, and the serverless cloud alternative to those is presented through AWS EventBridge. EventBridge allows us to create a periodically triggered rule using the same scheduling syntax as the traditional cron. I have had great success scheduling a Lambda function to run periodically to check statuses of other APIs, delete old database records, and replicate data to durable storage. EventBridge can be integrated with Step Functions to handle long-running cron jobs that are complex enough to require an orchestration tool.

Optimizing workloads in the cloud means learning about asynchronous processing and event-driven architecture. My preferred service for producing events for event-driven architectures is using SNS. SNS helps deliver and fan out events asynchronously. A pattern that I commonly use is emitting events from Lambda functions that I have integrated with API Gateway. Depending on the execution success of the API depends on the event(s) emitted from that client-facing Lambda. I will create Lambda functions as needed to handle the various types of events in the background. This allows me to reduce latency for my API's critical path but also complete processing in the background.

Data science and analytics is a hot topic nowadays, and moving data around can be finicky. AWS Kinesis is a service that allows multiple data sources to feed into the same Kinesis stream, and Kinesis delivers that data to its final destination. Lambda comes into play if anything in the data needs to be transformed before reaching its final destination. Kinesis will invoke the configured Lambda function with a group of data records to be processed. The Lambda function can perform any custom logic needed before returning the transformed records to Kinesis and ultimately their final resting place. Some ways I have used Lambda functions to transform Kinesis records are by scrubbing data from logs, standardizing data, injecting information, and performing real-time analytics on that data.

The final integration that I want to highlight is CloudWatch Alarms. CloudWatch is AWS's logging and monitoring solution and it comes with a few spin-off services including CloudWatch Alarms. We can build alarms based on logs or operational metrics that trigger at a predetermined threshold. If you have read the previous sections this might sound like something familiar: an alarm is an event. Using alarms in an event-driven architecture is a natural next step and once those alarms are triggered we can set them up to invoke a Lambda function. After that, the sky is the limit. We can write code for the Lambda function that dispatches messages, tries to remediate the problem on its own, trigger another service like incident management alerting, or anything else we can imagine.

This concludes the Lambda function integrations that I wanted to highlight, but there are plenty of others that I have not used or potentially even heard of. The sky is the limit here. Of all the services that are discussed throughout this guide, Lambda is my favorite. There is a cult-like following around it and for good reason. Lambda has shifted the way that we can write code and build applications. Serverless has opened a whole new door by creating a new paradigm and Lambda is the flagship function-as-a-service offering out of all the cloud providers. Learning how to effectively and efficiently write code for Lambdas and use Lambda functions is an incredible experience in and of itself.

## API Gateway

Creating a Lambda function is great, but once it is created, we need to create entry points for clients to access it. Normally, software is exposed to users through APIs, and naturally, AWS has a service that makes exposing resources as APIs easy. Enter Amazon API Gateway. API Gateways are a concept on their own, and AWS offers an implementation of an API Gateway bearing the same name. The concept behind API Gateways is to create a single entry point for all of an application's APIs. The API Gateway then routes traffic to the appropriate service.

Of course, the depth and full capabilities of an API Gateway are defined by the implementation itself. Amazon's implementation is simply called Amazon API Gateway. Another well-known API Gateway implementation was created by Netflix and is called Zuul.

As with any AWS service, Amazon API Gateway (from here on out I will refer to Amazon API Gateway as simply "API Gateway") is easy to integrate with other AWS services including Lambda. I would argue that API Gateway is the most used entry point to a Lambda function, and it is the go-to for creating APIs.

The way that I choose to use API Gateway and Lambda is similar to how a lot of boilerplate code for basic servers is set up. When creating a server the routes and methods are defined and functions are attached to them to handle their requests. I like to think of API Gateway as being the boilerplate code that sets up routes and methods, Lambda functions as being the logic that handles each request, and the integration between them as being the function wrapper or decorator.

API Gateway has a few resources that are the building blocks for an API's structure. An API resource itself (the resource is also called a Rest API or HTTP API) is an entity that separates a particular API Gateway instance from other APIs and is accessible by its own URL. The API is made up of various resources which correspond to API paths. Each path resource has methods attached to them which define which HTTP verbs can be used to manipulate the resources held behind it.

### API Gateway Building Blocks

API Gateway starts with a logical entity of an API. In my example which parallels an API Gateway and Lambda function API to a traditional server, provisioning an API resource would be the equivalent of starting a new server without any routes to handle requests. There is beauty in the orchestration behind this though. Before cloud providers and the easy services that they offer, provisioning the equivalent of an AWS API Gateway API resource would include finding a suitable server in a (most likely on-premises) data center; creating a new VM; installing the necessary libraries, languages, and packages; opening up ports; configuring DNS servers and networks to point to your new server; writing boilerplate API server code; packaging and deploying that code onto the VM; and finally running that code. I am positive that I am overlooking multiple steps for a more traditional scenario, but the point is for us to realize just how easy AWS makes these things.

After we have provisioned a new API resource within API Gateway, we can start defining the resources that make up the API. API design is a guide unto itself, so I will assume you already possess that knowledge or will acquire it elsewhere. There should not be any tricky parts here unless proxy resources are thrown into the mix. Proxy resources are a way to catch any routes like a slug route would. I have used proxy/slug resources in the past, but I try to stay away from them out of personal preference. While configuring a proxy resource on an API Gateway is not necessarily difficult, it does add a little extra overhead in the Lambda function that needs to handle the request.

The last resource needed in API Gateway before we can finally integrate with a Lambda function is the method resource. Method refers to any HTTP verb that will be used to interact with an API resource like GET or POST. The method itself is what integrates with a Lambda function because the method has all of the context required to finally know what action a user wants to take on a resource. API Gateway also allows us to configure a single integration with all HTTP verbs if that is desired. At this point, our API will have been designed and provisioned in API Gateway; however, there are still a few more actions we need to take before our API is live.

By the time we have a designed API, we will be ready to deploy that API into the world. Provisioning the API in API Gateway and configuring its resources and methods does not mean that your endpoints are live yet. To handle going live, API Gateway provides two more concepts: stages and deployments.

Stages are meant to represent an API during different phases of development. For example, an API could have dev and prod stages that are integrated with different Lambda functions. The dev stage could be mapping to experimental code or new features, while the prod stage could be mapping to the last known stable state. The name of the stage will be a path component preceding all other defined resources. For example, an API with a single `/user` path would have a dev stage path that translates to `/dev/user` and a prod stage path as `/prod/user`.

While stages can be useful, the way that API Gateway's pricing works (I will discuss pricing later) there is no penalty to creating two APIs with one being a dev instance and one being a prod instance. I prefer having two separate instances rather than two separate stages. Continuing along the route of having two separate APIs still means we need to create stages, but we would only need one stage per API instead of multiple stages on one API.

Deployments are the final piece of the puzzle before an API goes live. Stages and deployments must be associated together so that a deployment knows what API configuration needs to be deployed. A deployment is considered a resource in AWS, but what it really does is take the current state of an API and deploy it. My guess as to why AWS considers the deployment itself a resource is because those deployments are immutable. Immutability in AWS must mean that internally they need to capture and store it as a resource with a certain set of configurations.

If an API's configuration is changed by adding new resources or methods, or changing which Lambda functions are integrated with the API, then the API will need to be redeployed. This is simply a caveat of the deployment resource in API Gateway and its immutability. This used to trip me up because I could make a change to an API, see that change reflected in the console, but not experience the change as a user. The reason was that I needed to create a new deployment. The new deployment will not overwrite any URLs, but there is an option to change the stage that a deployment is associated with. The new deployment can be associated with the same stage, a different but existing stage, or a completely new stage. Remember that what will change is the stage name being reflected in the API's path, so a new deployment associated with a different stage could be a breaking change.

### API Gateway Endpoint Types

Since we now know how to deploy an API, there are a few options related to deployments that are worth discussing. As I have previously discussed AWS maintains data centers all over the world. With that type of infrastructure in place, deploying a customer-facing application can be more complex than simply "making it available on the internet." There are three geographically important ways to deploy an API Gateway and they are privately, regionally, and at the edge.

A private API Gateway deployment is not something I recommend or use often. I have created a few private APIs, but there are downsides to privately deployed APIs. The largest downside to me is that privately deployed APIs are only available from within a VPC. Remember the small VPC section that we discussed earlier? It was small for a reason and that is because VPCs require more maintenance and overhead to get going and cost more money depending on the VPC setup. If security concerns are what make privately deployed APIs attractive, there are security layers that can be added to API Gateways like API keys and Lambda authorizers, which I will go over soon.

Regional APIs are publicly available APIs with domain names. Regional APIs are a great option for customer-facing APIs and they are deployed on a per-region basis. Deploying regional API Gateways in multiple regions is a great use case for CloudFormation since CloudFormation Stacks are also created on a per-region basis. We also have the option to give regional APIs custom domain names, which are not available for privately deployed APIs. A custom domain name needs to be mapped to a specific API and stage to be accepted by the API Gateway. A CNAME DNS entry can then be made pointing to the API Gateway, which can either be handled by AWS Route 53 or any other DNS service.

The last API Gateway deployment option is an edge-optimized API endpoint. Edge-optimized APIs are automatically deployed to AWS's edge locations, which also exist all over the world. This is the default API deployment type and is best suited for APIs with a global user base. There are no different billing models associated with the different deployment options, so I suggest sticking with the default edge-optimized option. Edge-optimized endpoints can have custom domain names similarly to regional ones with the one custom domain being routable to any edge location. AWS does perform some magic here, and this is one clear example of the power that anyone can wield while using AWS.

### API Keys

Generating API keys as a way of providing some level of security for an API is a common enough practice that AWS made it into a service. I want to stop and note that API keys do not solely make an API secure, so make sure that you know and understand potential security concerns before only relying on API keys. To associate API keys with an API we need to run through a few steps first.

Our API Gateway methods first need to know that they should be expecting calls to include an API key, so there is a simple switch that we need to turn on telling methods to require API keys. Remember that if this change is being made after having already deployed an API, that API will need to be redeployed. This is the first piece of the puzzle.

Next, we need to create something called a usage plan. This resource is exactly what it sounds like. We can configure plans with quotas and throttling limits that we can give clients. Keep in mind that "clients" here can be customers or developers. Usage plans will need to be associated with an API's stage so that API will ideally already have a stage created before going down the path of creating keys for it. I like to keep usage plan limits somewhat restrictive at the start in case anything is ever compromised, and I scale those limits up as needed whenever traffic starts coming online for a particular API.

Finally, we can create our API keys. AWS will generate a random string to serve as the API key and will be expected to be passed to the API as the value to a header called `X-Api-Key`. An API key will need to be associated with a usage plan before it will be accepted. What this all means is that a deployed API can require API keys for certain methods. Those methods which require keys will employ throttling and quota limits on the users that own those keys.

I create API keys for all of my APIs because it gives me an added layer of comfort knowing that attacking my APIs would be a little more difficult. Again, I am talking about security lightheartedly here, so when it comes to building APIs at an enterprise level, be sure to check with a security team before thinking that simple API keys are the only security measures that need to be taken.

### Lambda Authorizers

While API keys are a nice built-in way to check that a user is allowed to access an API, AWS offers an option for a more custom approach. This is yet another way to integrate Lambda functions between AWS services called Lambda Authorizers. The idea here is that a request first invokes a Lambda function, which either accepts or denies the request based on some sort of custom authorization scheme. If the request is denied, then API Gateway returns an error code to the caller. If the request is accepted, then API Gateway forwards the request to the appropriate resource and method to be handled normally. Authorizers also grant us the option to cache the Authorizer Lambda's response, which puts less load on the Lambda authorizer, reduces latency, and reduces costs.

Two different types of authorizer event payloads can be configured which use different parts of a request to authorize a user and cache the response. The first type of event payload is called "token," and token types are configured to look at a configurable header value. The second type of event payload is called "request," and request types can be configured to look at other parts of a request such as header values, query string values, stage variables, and other context. Both types cache based on the value they are configured to look at. For example, requests that come in with the same authorization header values will first be checked against the cache for the last decision against the same header value and only forwarded to the Lambda Authorizer if there is no cache hit.

Authorizers even let us write a regex for initial validation before sending it to the Lambda, so if a malformed token is sent in it is automatically rejected without using Lambda execution time. Authorizer Lambdas receive the configured values and must construct and return an IAM policy, which probably sounds more complicated than it is. There are numerous examples online from both AWS and third-parties with proper code to handle IAM policy construction, so do not be deterred from using Authorizers because they involve IAM.

My personal preference when it comes to Lambda Authorizers is using the token type of event payloads configured to look at the standard Authorization header. This pulls out the Authorization header for my Lambda and caches the exact token so my Lambda does not need to run every time the token's owner calls my API. I use Bearer Tokens to authenticate users, so I pull the JWTs out of those header values to be verified. Once a user is verified, I pull any additional context in to construct the IAM policy and return it.

Another fun bit about Authorizer Lambdas is that we can return extra context which will subsequently be passed to the Lambdas integrated with our API. I find that any code or information that all of my handler Lambdas need is best found in the Authorizer Lambda, which is cached and passed down to subsequent Lambdas. Since that code would have to be run anyway, I might as well run it once and then let API Gateway's cache take over for a performance boost.

### API Gateway Proxy Integrations

API Gateway and Lambda are a match made in heaven, but API Gateway can integrate with other AWS services directly like DynamoDB or SNS. Options open up for quicker development times and easier maintenance if there is no custom code between the API Gateway and the end service. It is not uncommon to see the trinity of serverless operating together: API Gateway, Lambda, and DynamoDB. Sometimes we don't even need to write code for a Lambda function though. If our code is simply translating the request to a DynamoDB row and inserting that item, then we have a great use case for a direct API Gateway proxy setup.

All AWS services are exposed through APIs, so we can expose the AWS APIs to our users with a thin layer between being API Gateway. We can restrict access to those AWS services and offer a more intuitive interface. An API in API Gateway can be set up the same way as we would with a Lambda function except that we instead configure the API's integration type as an AWS Service Proxy.

Once the API is configured as a proxy, we can map user input to the other AWS service's API input using special syntax for this purpose, so there is a slight learning curve here with the mapping template syntax which is called Apache Velocity Template Language.

It is worth pointing out that a proxy integration without server-side code (i.e. a Lambda in between) means that we lose the ability to validate and scrub input. This is simply one of the trade-offs that come with the simplicity of an integration like this. Nevertheless, I have still had success implementing these types of integrations.

My personal preference regarding API Gateway Proxy Integrations is to use them sparingly and for internal use. The largest downside for me is the lack of input validation, so I prefer to present a user with an API that allows me to validate input, then use a proxy integration for non-user-facing-services. This makes sure that I will have had the chance to validate and scrub input by the time it gets to an internal service that works as a proxy integration. The largest benefit to the proxy integrations is that they allow us to provide a thin translation layer of sorts instead of dealing with AWS's APIs, which can sometimes be confusing especially to developers who are not familiar with them.

### Pricing

Pricing for API Gateway is similar to Lambda where we only pay by valid requests. I mentioned earlier that I like to provision one stage per API in API Gateway. I can provision those extra API resources without any extra cost because I still only pay by request instead of provisioned API.

The pay-by-use model is what makes serverless extremely attractive to me as well as a multitude of other developers and teams. A traditional setup would most likely charge me by instance because one API means one VM and I would be charged per VM since it would need to be up and running in a data center at all times, which costs money. AWS and its serverless offerings make much better use of their overall computing power, which is passed down to us in the form of more efficient billing models.

I mentioned that with API Gateway we only pay by *valid* requests. This is the reason why I brought up some of the authentication features of API Gateway like API keys and Lambda Authorizers. If a user hits an API with invalid keys or a Lambda Authorizer denies access, then that request does not get charged against us. Granted with a Lambda Authorizer, we would still pay for the Lambda's execution time. However, I see this as a great benefit to us. I can sleep well at night knowing that if some web crawler or rapid-fire script finds my URL, it will not go against me if I have set up API keys or a Lambda Authorizer.

Keep in mind that while serverless offerings like API Gateway provide attractive prices for sporadic traffic, they may or may not be better in the long run with more stable traffic levels. The cost of serverless over the long run is something that is debated online, so it is up to you to determine whether or not you and your traffic will see a price benefit.

For such a seemingly simple service, API Gateway has a lot to offer. As an entry point for your clients into AWS and as a tool to enhance internal workflows. There is more to be learned about API Gateway, but this is probably more than enough information to get anyone up to speed and off to the races.

## DynamoDB

Using API Gateway and Lambda together opens doors to a multitude of possibilities, but there is a missing piece in that simple integration which is persistent data storage. DynamoDB is my preferred service to store data just as it is for numerous other developers all around the world.

A traditional alternative to DynamoDB is a NoSQL database like MongoDB but MongoDB is not even "traditional" to most people. An old-school alternative to this is a SQL database like MySQL or PostgreSQL. The goal here is to allow our application to store user data without needing to rely on our application servers. DynamoDB and SQL databases share a common purpose of durably storing data, but be warned, modeling that data and the operations that can be performed on that data are completely different. DynamoDB is just as different from a SQL database as it is from MongoDB, so prior knowledge in using NoSQL databases does not necessarily preclude someone from the learning curve that is DynamoDB.

Before I go too in-depth with DynamoDB, I want to explain in simple terms how I use it. DynamoDB is marketed as a key-value and document database that is lightning quick at any scale. This is a purpose-built data store that runs some of Amazon's services, so it naturally has to be highly available, durable, and fast since they pride themselves on a low latency user experience. All of those benefits mean that there are some downsides though like trickier data modeling and inflexible querying. Luckily, there is enough of a community surrounding this technology that there are some methods and best practices to mitigate some of that downside.

While DynamoDB is a database when we interact with the service we do not provision a database. We can ask DynamoDB for new tables, but AWS handles the database cluster, autoscaling, throughput, and other lower-level database configuration. While this might seem like a downside to some I view it as an upside. I would rather provision a table and start storing data than have to worry about piecing together a database cluster.

### NoSQL

I assume that most people who have stumbled across this guide will have had some experience developing applications or provisioning infrastructure, and my guess is that databases have crossed anyone's path in the software and technology realm. The majority of database engines are SQL engines so switching from something like PostgreSQL to SQLite would not be too difficult since most of the querying concepts are the same. However, switching to a NoSQL database might involve some level of challenge.

The main difference between SQL and NoSQL databases is that NoSQL databases rely on querying by keys instead of any arbitrary column that could be matched like in SQL. While I do not want to focus on the differences between the two styles of databases, I do think it is important to point out that they are completely different ways of storing and retrieving data.

### Partition Keys, Attributes, and Scans

That main difference also brings up one of the most relevant observations I have had about DynamoDB, which is that all data needs to be queried by its key. This was one of the disadvantages and stickiest points for me while I was learning how to use DynamoDB. With a SQL database, I could match any data with a WHERE, but with DynamoDB, I always want a key (also called a partition key) to retrieve data. There are ways to grab items in bulk, but those methods are much more inefficient compared to querying by key. Using a partition key that identifies an item gives us the option to operate on that single item, and we have access to normal data operations like creating, reading, updating, and deleting those items.

Creating and operating on an item through its partition key is easy enough, but there is one aspect that I want to bring up about these simple key-value items before we go more in-depth. Thinking of keys is easy enough, but the way that DynamoDB handles the values associated with those keys is a little more difficult. Particularly there is no schema and the types are built into the value itself. I like to think of the value as a simple object or dictionary. This way of thinking makes not having a schema nothing out of the ordinary. In languages like Javascript or Python, we also do not need to declare the types of the values we want to store before we store them.

The trickier piece of DynamoDB values is encoding the data types of attributes. There are a select few data types including strings, numbers, booleans, lists, and maps, to name a few. The value of an attribute needs to be properly encoded for DynamoDB to accept it. One easy way around this is to use the AWS SDK for Javascript, which includes something called the Document Client. Document Client allows us to pass in a normal Javascript object and the client will encode the payload for us depending on the value's native Javascript type. There is more documentation online, but I highly recommend using Document Client over encoding the payloads yourself.

I would like to go a little deeper into how DynamoDB works before discussing bulk item retrieval methods. DynamoDB can scale horizontally which means it can take on a much larger scale than a SQL database. Because of its horizontal scaling, the data stored in a DynamoDB table is not necessarily all stored on a single machine. That data is "load balanced" in a way such that no single machine is taking all query requests (or at least that is the intent). The way that the data is partitioned is using an item's primary key, also known as its partition key. For this reason, using a partition key with DynamoDB is the fastest way to retrieve data, in the same way, using a hash map key is faster than iterating through all of the hash map's values looking for a particular match. One way that I think about DynamoDB is simply that: a huge distributed hash map.

With that knowledge, we can deduce why there is query inflexibility within DynamoDB. Querying without a partition key means DynamoDB has to search through all of a table's items to find items that match what we are looking for. It simply is not optimized for that kind of search and instead gains benefits when searching by key. Having a key to query by means DynamoDB will know exactly which machine the item is stored on and the machine, or partition, will know exactly where to find the data.

The `Scan` operation is one type of bulk item retrieval that DynamoDB offers. A `Scan` looks through all of the items in a table and returns those that match specific filters. Where a direct read of an item by partition key would result in a single item returned, a `Scan` can return multiple items based on partition key patterns or other attribute patterns. This is one way to get around the inflexibility of DynamoDB table design, but I highly recommend against using this operation. Since a `Scan` reads through all items in a table, we have to pay for each one of those read operations which adds up quickly. Reading each item in a table means that the cost (in time and money) of a `Scan` increases as the items in a table increase. If a `Scan` is required, try to make the operation run only periodically instead of per request.

### Sort Keys and Queries

It is possible to get far with only partition keys but eventually, data modeling and retrieval options will get too constrained using only the partition key. Another layer on top of partition keys is sort keys. Sort keys are also known as secondary keys and they are a way to store multiple items under the same partition key. What this means is that we can tell DynamoDB which items we are looking for with more granularity.

With the introduction of sort keys complete, there is another DynamoDB bulk retrieval operation that is worth bringing up to highlight a little more flexibility. A `Query` operation allows us to look through all of the items under a given partition key. `Queries` are a compromise that tries to inject more query flexibility while also keeping speed a top priority.

As a side note, so far when I have said "query" I mean simply requesting DynamoDB for data. With the introduction of the `Query` operation, I can see how the concept of querying and the DynamoDB-specific operation of `Query` could get confusing, so I will capitalize the operation (`Query`) and keep the concept lowercase (query).

`Query` operations in DynamoDB are similar to `Scans`, but instead of reading all items in a table, a `Query` will only read the items with a given partition key. This adds flexibility to queries and reduces the number of read operations in comparison to `Scans` because we only read a subset of items in a table. For a `Query` to properly work, we need to pass it the partition key for the items we want to read through and any filters or expressions we want to match the secondary key or attributes against. I like to think of a `Query` like a miniature `Scan` on a partition key instead of an entire table.

One strategy that developers including myself have used to create useful sort keys for `Queries` is concatenating multiple pieces of identifying information to create a sort key. Conventionally, that information is joined with hash signs (`#`) to create some sort of division that uses an uncommonly used character.

For example, let's say we wanted to keep track of a user's orders and we wanted to be able to retrieve completed orders for given time bounds. For this example, I will assume that there are different states that an order can be in such as pending or completed. Our partition key could be an identifying piece of the user's information like a unique ID or email address. Our sort key could be the state that an order is in, appended with a hash (`#`), appended with a timestamp. This allows us to `Query` our given partition idea looking for sort keys starting with the order state and beginning of the desired time and ending with the order state and end of the desired time.

Some of the compromises that come with DynamoDB are brought out with that example and are worth discussing. The `Queries` that can be made on those items with that data model are limited. For example, it would be difficult or expensive to `Query` for all types of orders on a given date since `Queries` do not have an "ends with" filter. It would also be difficult to find an order based on its order ID since the order ID is part of neither the partition nor the sort key. These are compromises that can be worked around with properly defined access patterns and a data model to fit.

`Querying` for all order types based on timestamps would be difficult. The brute force method would involve `Querying` for all possible states between a timeframe, which is similar to the original example with the added expense of performing this operation for every potential state. While a brute force effort could work, this is a better example of needing to clearly define access patterns before modeling our data. It should be known that this type of access pattern would not be as well supported as others.

`Querying` for an order based on an order ID could also be worked around by adding that order ID to the beginning of the sort key. The downside to this approach is that locating orders would require an order ID from the start instead of less specific information like the state of the order. Again, these are known downsides that should be discussed before implementing a data model.

I have found success by thinking of partition keys and sort keys as a form of scope that narrows. A partition key needs to be the largest and highest order abstraction in a data set. It needs to be in the highest order because it is the entry point for us `Querying` our dataset. After the partition key is determined, I like to create sort keys in order of narrowing scope. In the original example, the partition key of user ID would always need to be known and is the highest order abstraction in our dataset. Luckily, if a user is logged in, then we will be able to grab their ID from the context of the user's request. After that, the sort key was created with the order state being the next most narrow scope followed by the timestamp based on the access pattern. The narrowing scope of a sort key needs to be factored into the access pattern we want to enable.

DynamoDB table design for keys and proper access patterns is a deep deep hole. There are books written entirely about this subject that have the scope to go into much more depth than I want to in this guide. As your familiarity and use case complexity increase I suggest seeking out that information. For now, this should be a good primer and get you on your way to working with DynamoDB.

### Global Secondary Indexes and Data Duplication

Access pattern options are limited based on partition and sort keys. DynamoDB does offer another feature to help get around access pattern deficiencies though. Global secondary indexes (GSI) are a way to create a new set of partition and sort keys. The idea is that DynamoDB will automatically replicate data but use different attributes to create the partition or sort keys and fill those new items with whatever attributes are configured to copy over.

What this means is that we are not entirely constrained to the limited access patterns offered by a single partition and sort key combination because we can create multiple combinations based on the same set of data. As of yet, I have rarely used GSIs, but I do know that they have their place especially in more complex services. This feature would not be offered if there was not a valid use case.

Global Secondary Indexes also highlight another mindset shift for data storage which is not giving effort to data deduplication. Rick Houlihan is an idol in the data world, and he works on DynamoDB at AWS. He has given priceless talks at AWS re:Invent (which I heavily suggest watching to learn more about DynamoDB), and one of the more interesting pieces of one in his talks was about the evolution and pricing in computing that led to NoSQL. He explains that at first storage was expensive, but now storage is cheap and commoditized. SQL databases were great for expensive storage because that data only needed to be written to disk once and then provided numerous ways to query against itself. Cheap storage dissolves that requirement, so duplicating data is feasible from a financial standpoint.

Duplicating data in a SQL database is considered a bad design, but duplicating data in DynamoDB is natural and supported in native features. Instead of issuing multiple queries to retrieve all the data needed in an operation, duplicate the data in a way that only requires one `Query`. While this might sound counterintuitive it is an AWS best practice.

### Global Tables

While we are on the subject of replicating data, there is a DynamoDB feature that enables geographically diverse database replicas. DynamoDB Global Tables handle replicating data written in one AWS region to all other AWS regions in which the Global Table configuration is set up. The table name and APIs are the same as a non-replicated DynamoDB table. Using my suggestion of interacting with DynamoDB through the SDK Document Client means that enabling this feature is as simple as configuring it in DynamoDB and pointing the SDK to the region closest to where the code is deployed.

While Global Tables are not always necessary, growing applications with a global user base will most likely need it or at least see a performance benefit from using it. Deploying API Gateways and Lambda functions into the regions where a Global Table is replicated means that we can give users the best latency by bringing a copy of our application to them. Since our Lambda functions should be written without state, DynamoDB is the only data that we need to make sure is consistent, and luckily, Global Tables offers us that consistency out of the box. Keep in mind though that we are talking about eventual consistency. Amazon (and therefore AWS) prefers low latency which is why Global Tables replicate asynchronously. For more information about what that means, I suggest looking into the CAP theorem.

### DynamoDB Streams

I have brought up event-driven architecture already in this guide, and I want to extend that discussion to DynamoDB. DynamoDB Streams is a feature that helps enable event-driven architecture and is necessary for Global Tables. Streams are a way to emit events about data manipulation on a table. Global Tables use Streams to know when data needs to be replicated, which is understandable once you think about it.

Streams can be thought of as SQL triggers in a sense. Whenever an event occurs on an item in a table, a record is written to the DynamoDB Stream which can be consumed by another service such as a Lambda function. This further enhances the possibilities of what a fully serverless architecture can become. The Lambda that reads from a Stream can trigger a notification for another Lambda function to run asynchronously, push that data to long-term storage, or anything else we could come up with. As I have mentioned before, Lambda is the glue between AWS services, and Streams are another example of just that.

While I have not come across a use case for using Streams in my projects, I feel that it is worthwhile to bring it up. A use case that I am keeping my eye out for is something along the lines of triggering an asynchronous workflow based on specific written data from a Stream instead of triggering that notification or event from a critical path Lambda. This method would reduce latency by not needing to produce the event in a user-facing Lambda, but instead produce the event based on data written to DynamoDB.

### Integrations

Now that we have discussed Lambda, API Gateway, and DynamoDB, we have the basic underpinnings for building serverless application infrastructure. These three services can take anyone a long way, and in the serverless AWS world, these are the trifecta of serverless technologies. Lambda integrates well using DynamoDB as its datastore. Since they are both quick and flexible pieces of infrastructure, a Lambda function can be spun up with a DynamoDB table much quicker than more traditional infrastructure like an EC2 instance and an RDS cluster. We simply need to tell AWS that we want a new lightweight compute platform and database table and AWS handles the rest. Since the AWS SDK is built into Lambda runtimes, we only need to import DynamoDB SDKs to start building with both technologies.

I briefly mentioned using DynamoDB as a cache in the Lambda section, and I wanted to expound more on that here. First, DynamoDB is not a cache, it is a database. There is a native service called DynamoDB Accelerator (DAX) which is a purpose-built, in-memory cache that plugs right into DynamoDB. However, there is no such thing as a serverless cache (at least as of me writing this). The closest that I have come to seeing a serverless caching solution is simply using DynamoDB. It comes with single-digit millisecond latency, which my projects have shown me to be true in practice. When the scale that DynamoDB can reach is taken into consideration, that kind of speed is wild. So while it might not be an actual cache, DynamoDB is probably the fastest and most durable datastore out there while also being fully managed and priced like a real serverless product.

The final integration that I want to bring up is the direct integration with API Gateway that I mentioned in the API Gateway section. API Gateway can integrate with quite a few other AWS services out of the box, but the DynamoDB integration is a useful one. I have seen projects that involved writing code for a Lambda function that sat between an API Gateway and DynamoDB table that read in the API Gateway data and wrote it to DynamoDB, which could have been completely circumvented with this integration. Using DynamoDB as a cache can also be slightly easier by using an API Gateway in front of that "cache." I have personally used a setup of an API Gateway directly integrated with DynamoDB to be used as a common cache. It functioned as its service because I used a Lambda on a cron to populate the DynamoDB table so that the values were always updated whenever another service needed that "cached" data.

And just like that, we are done with our DynamoDB discussion. Since we have discussed the serverless trifecta, anyone who has read this far should have a decent foundation for creating serverless application backends. This might be the extent of the necessary discussion for the majority of engineers out there, and if that is the case for you, congratulations. I hope that you can take this knowledge, stitch it together in your own unique way, and create wonderful services.


## S3

Amazon Simple Storage Service (S3) is a persistent file storage service and a really good one at that. While I am not privy to the actual inner workings of Amazon data centers, I imagine S3 is simply a huge cluster of hard drives spread out across every data center that Amazon owns. The generic name for a file storage solution like S3 is called Network Attached Storage (NAS), but I can not imagine another company having a storage solution quite as large, durable, or sophisticated as S3. I would not want to begin taking a guess at how much data is stored in S3 across all of the applications using it. AWS often touts that S3 has eleven nines of durability which means a file will almost never be lost.

When we interact with S3, we do so using foundational entities called "buckets" and "objects". Each S3 bucket is identified by a globally unique name (more on that later) and carries its own policies and isolated file structures from other buckets. I like to think of a bucket as a single hard drive although in actuality it is most likely a virtual drive or partition. However, that hard drive can scale from bytes to terabytes in a matter of seconds, practically never lose files, and provides quick reads and writes. Files are represented by their foundational entity called objects and can naturally be created, read, updated, and deleted. At its core, that is all S3 does; it reads and writes files. But just like any other AWS service, there are more options and features that can be enabled to make S3 do wonderful things.

### Versioning

Before we get too into the weeds, I want to bring up versioning. Most AWS services, including S3, that take in data will happily retain that data by default and charge us for storing it. There are ways to automatically delete old data that is not being used, and S3 versioning helps us do that. Another service that loves to retain data is CloudWatch, and I will also bring up a strategy to reduce storage costs with that service.

Versioning objects (or files) in S3 is useful outside of automatic deletion to save space and money. By enabling versioning we tell the S3 bucket to retain the history of a given object by name including when it was deleted. If an object is overwritten, we can still download or restore an old version. If an object is deleted, the deletion becomes a simple marker in time without deleting the entire history of the object.

I also prefer versioning deployment packages (which are simply `zip` files) for Lambda functions. We can pass the version ID to a CloudFormation template that builds the Lambda function, and Lambda grabs the exact version of its deployment package from S3. Doing this allows us to update a function only whenever we are ready and we declare the upgrade. Versioning deployment packages also gives us an easy option to roll back changes in the event of a fatal bug. In my opinion, versioning deployments no matter what the code is delivered in (`zip` file, Docker image, etc.) is a must because it allows developers to know exactly what code is running where, and it allows for easy rollbacks.

Out of preference, I enable versioning on almost every bucket I create now. Not only buckets that hold deployment packages, but also buckets where I store files in case one is ever accidentally deleted.

The space and cost savings comes into play with the introduction of lifecycles. Lifecycles in S3 pertain to objects and we can "expire" or delete those objects whenever they meet certain conditions. My favorite method of deleting objects is based on time and something that S3 calls "noncurrent versions." By setting an S3 bucket up this way, I automatically delete past versions of an object after a certain period of its inactive use. I normally set up the deletion of an object 30 - 90 days after it becomes the noncurrent version. This allows me to make sure that the new version of a file is valid and I do not need to roll back (especially in the case of a deployment package).

Lifecycles are useful for a wide variety of cases though. We can also use lifecycles to transition objects to different S3 storage classes. Objects are identified for a transition to a new storage class or expiration by either the object's name, path, or tags. I have used lifecycles to expire financial reports after they no longer need to be retained and to move large amounts of data to a deeper and cheaper storage class after they no longer need to be fresh for data analytics. As with any other AWS feature, the options are practically endless.

Storage classes are a new concept that I glossed over before bringing them up in the context of lifecycles, so I want to swing back around to those. The last paragraph might make more sense after reading this one. Storage classes are the way that S3 defines how readily available objects are. Objects can be stored in "deeper" levels of storage which makes persistent storage much cheaper, but retrieval times increase. If we needed to store a large amount of data for record-keeping but we rarely if ever access that data, then we can store it in a deep class of S3 (S3 Glacier or S3 Glacier Deep Archive) so it does not cost us as much.

### Hosting Websites

One extremely common application of S3 is to use it as a web server. Since web pages are really just HTML files and S3 is a great way to store files, it seems like a natural fit. Hosting a website out of S3 benefits from the globally diverse set of data centers that AWS runs, and end-user latency can benefit even more from the integration between S3 and CloudFront (more on that later in this section).

In the introduction section, I mentioned that all S3 buckets need globally unique names and website hosting is the reason. Unless a custom domain sits in front of the S3 bucket, AWS hosts the website using the bucket's name to give it a unique domain name. The unique bucket domain name can be the destination of a DNS CNAME entry either in Route 53 or any other DNS provider to add a custom domain instead of using the default AWS-assigned name.

One caveat of hosting a website out of S3 is that S3 only provides static file hosting with no compute behind it. That means that we are not capable of hosting a server-side rendered website out of the box. This is more relevant for old-school server-side scripts like PHP and ASP.NET and newer frameworks like Next.js. There is an AWS native way to support server-side rendered sites using a service called Amplify, but that is not something that I plan on covering in this guide.

With the server-side discussion out the way, S3 website hosting is still likely to cover most needs like static site hosting or single-page applications. I use Jekyll for my personal website, which is a commonly used static site builder. I run a build command and it spits out the HTML and CSS. Jekyll and Gatsby are two common static site builders that would integrate well with hosting a website out of an S3 bucket. Static site builders are simple to set up and hosting out of an S3 bucket is also simple to set up, so this is my recommendation for a straightforward website unless you need Javascript.

Single page application (SPA) frameworks like React and Vue can also take advantage of hosting their static build pieces out of an S3 bucket. The result of their build processes is what would live in S3 and then an API consumed by the front end would need to exist in something like API Gateway and Lambda. I have personally seen this type of setup work well with React, but I can not speak to any other front-end frameworks. If your front end needs an extensive amount of Javascript, then I recommend creating a SPA to host out of S3.

The last aspect of hosting a website out of an S3 bucket that I have mentioned is integrating with CloudFront. CloudFront (and all CDNs for that matter) is made for delivering static content quickly and at scale. Whether that static content is an image, video, or HTML file does not matter. For this integration to work, we need a new CloudFront distribution with our website's S3 bucket as content storage. That is it. S3 and CloudFront are about as easy to integrate as AWS services get.

### Data Analytics

S3 is a great platform that lends itself to a wide array of use cases, but one use case takes the cake for needing scale and that is data analytics. This is about as trendy as I want to get, but combing through logs and metrics at scale is a task that just about every well-used application will need at some point in its lifetime.

When applied to data analytics S3 can help us build a data lake for big data analytics, and we can then use another serverless service called Athena to write queries against that data. (Sorry for throwing all those buzzwords at you in a single sentence.) I have personally used this combination and had success with it. However, this is more applicable to large workloads with numerous users, so it might not be as easy to test out on a small-scale application since it works best with mountains of data.

Athena allows us to query against large datasets without needing to provision servers or write special software to parse through data. There are some caveats though. Data needs to be in a predetermined format like CSV, JSON, or Parquet. Pricing is also based on the amount of data scanned, so as a dataset grows so does the cost to query it.

Combining data into the correct format before writing it to S3 can be tricky depending on the data's source and what it looks like before it is written. When it comes to moving data at a large scale one service called Kinesis stands out. I will talk about Kinesis more later in the guide, but for now just think of it as a pipeline where we can take in data, transform it to our liking, and write to S3. Kinesis is my preferred way of moving and transforming data before it reaches its final resting point.

I have personally experienced two different ways of transformed data formats before writing to S3 and querying using Athena and they are JSON and Parquet. Both are file formats, but the advantage of Parquet is that the data can be better compressed due to it being a columnar file format. Both file formats are equally as flexible with Athena when it comes to updating schemas. My recommendation is to use JSON if you are not storing a ton of data or do not want to mess with a new file format because most people are familiar with JSON. However, I recommend using Parquet if you are storing enormous amounts of data like sensor records from IoT devices every second. The advantage Parquet has over other file formats is that the aforementioned Kinesis has a configuration setting that will convert JSON to Parquet for us and compress it automatically if we choose to enable it.

### Final S3 Remarks

There are some final remarks that are worth mentioning about S3, but probably do not deserve their own section headings. The first is using S3 in tandem with Lambda. Using S3 to store Lambda's deployment packages is a natural integration, but I have found success using S3 to store files written and read during Lambda function executions as well. Lambda has a fairly well-known limitation for file storage in the Lambda process itself under the `/tmp` directory. Since S3 has barely any file storage limitations and it integrates easily through the AWS SDK, I commonly use S3 as the final resting point for files that I either need to read or write during Lambda execution.

Another aspect of S3 is presigned URLs. A service or IAM entity can create what is called a presigned URL that grants anyone in possession of that URL read and/or write access to the bucket for which is presigned URL was created. This means that we can allow non-AWS users to access buckets without needing to give them full-blown permissions or require them to integrate with IAM. Presigned URLs make file uploading and downloading super simple for a non-developer end user.

Lastly, there are some fun physical data storage solutions that I wanted to bring to light even if they are not technically S3 services. As a disclaimer, I have not personally used any of these, but I always get a kick out of seeing them. AWS has something called the Snow Family, which is a series of physical devices meant to be used at the edge and to migrate data.

The smallest is called Snowcone, which is a 4.5-pound hard drive with 8 terabytes of storage.

The next largest is called Snowball, which is a larger device that supports S3-compatible storage, some EC2 instances, and Lambda functions. It is like a small subset of AWS that can be brought anywhere.

The last and largest of the Snow Family is the Snowmobile, which is a legitimate trailer that needs to be pulled by a semi-truck used to migrate up to 100 petabytes of data to AWS. It is crazy to me that this exists. There are obviously use cases where a traditional data center wants to migrate to the cloud and it makes more economic sense to ship that data rather than upload it over the internet.

S3 is a versatile, scalable, and extremely durable object storage solution. If the built-in durability is not enough, there is also a disaster recovery option by configuring S3 to replicate data across global regions. I have used S3 in just about every single one of my projects, and I imagine that I will continue using it just as heavily in the future. S3 also gets brownie points from me for being the first AWS service I ever used. For anyone that has not explored this service yet, it is definitely worth looking into.

## CloudWatch

So far we have discussed foundational services that can help build a serverless application. Now we need to discuss an aspect that is just as important as building, which is observing. While building an application we produce all sorts of logs and in AWS those logs go to a centralized service called CloudWatch. At the same time, the infrastructure that our code is running on produces its own set of logs and metrics which also go to CloudWatch.

Another AWS service that I want to include in this section and discussion is called X-Ray. X-Ray provides tracing, which helps a ton with debugging timing and latency when a system starts getting more complex. X-Ray pieces together what API calls are made to what services and tracks the amount of time that each of those calls takes. It is a super helpful service to piece together what is truly happening within a system and find bottlenecks. We can enable X-Ray on Lambda and other services as a configuration item, and we can trace calls outside of AWS by patching HTTP libraries. There is more documentation about patching libraries online. I do not feel like this guide is the right place to give specifics because it would probably be quickly outdated.

CloudWatch functions similarly to a log aggregating solution like ElasticSearch, Splunk, or Datadog. There are caveats though as with any other service. AWS builds generalized services and common functionality with some being more of a specialty than others, and CloudWatch is not one of their specialties in my opinion. Spending some time in AWS forums will undoubtedly mean crossing paths with someone who deeply dislikes CloudWatch because of how slow it is. From personal experience, I can say that CloudWatch is indeed a bit slow. There is a noticeable lag between when logs are written and when they appear in the console, but unless you are dealing with extremely time-sensitive applications I do not think it is unbearable.

### Logs

One of the first reasons I familiarized myself with CloudWatch was reading logs. I remember making the obligatory "Hello World" Lambda function from a tutorial, invoking it, then needing to find where it logged. The way CloudWatch organizes logs is straightforward once you understand it but can be confusing if you never take the time to properly comprehend the different logging entities.

We will start with "log groups." A log group is simply a collection of logs from a common resource. For example, each Lambda function will have its own log group where all logs are written. For most infrastructure, including Lambda functions, AWS will automatically create log groups whenever logs are written. The logs need to go somewhere, but if we have already created a log group with the correct name, AWS will write the logs to the pre-created log group. A word of caution: always create log groups in your infrastructure as code before AWS creates them on its own. Letting AWS create a log group by itself makes it difficult to regain control from an infrastructure as code standpoint after it has been created.

The reason I suggest creating log groups from the start mostly pertains to pricing and data retention. I brought up a similar point in the S3 conversation that AWS loves to retain our data by default and charges us for it. In the same way, we can set up automatic object deletion in S3, we can set up automatic log deletion in CloudWatch based on log groups. If we let AWS automatically create the log group for us, then they retain logs forever by default even though we can manually change that retention period. And let me tell you, AWS charges a pretty penny to store logs compared to other data storage options.

Owning log groups through infrastructure as code also allows us to easily define resources like subscription filters and metric filters. Subscription filters are a way to treat logs like events and trigger an action based on those logs being written, but I will talk more about them in the Integrations section below. Metric filters are a way to parse logs and create custom metrics that will be treated like any other CloudWatch metric. I will discuss CloudWatch metrics more in the following section.

Log groups are a container for similar logs, but they are not the termination point where logs are normally written. Within log groups, there are log streams. Where log groups would represent a defined resource like a Lambda function, log streams would represent an instance that was spun up to handle traffic for that resource like an individual virtual machine where the Lambda's code runs. This might seem confusing at first, but after using these entities for a little while, it will all make sense.

Log streams make tracing logs through a system a little difficult because concurrent requests can result in interwoven execution logs. Imagine four virtual machine instances used to automatically scale up a Lambda function receiving lots of traffic. Each one of those virtual machines will write logs to their own log stream in the same log group. Finding which log stream a particular request's logs are written to ends up being a guess and check. Sometimes multiple executions handled by the same instance can intermingle, so determining which logs belong to which request is difficult. Getting around this situation becomes a little easier though with another CloudWatch service called Insights.

CloudWatch Insights is a better way to query logs across log groups. Insights is closer to resembling Splunk since it allows for more complex queries and querying across multiple log groups (or indices for Splunk) at the same time. Each request that hits a Lambda function is given a unique ID called a request ID, and every log written will automatically have the request ID injected into it. Finding and using the request ID is a great way to pick out specific logs from a single execution.

### Metrics

When we build applications, we create logs. When we provision infrastructure, AWS creates metrics. For almost every AWS service, there are corresponding metrics that are produced and available for us to consume. Metrics are the backbone of monitoring, and while they might not play as huge of a role in smaller projects, they are highly depended upon when an application services more and more users.

Most of the metrics are fairly standard and what we might expect to see from a normal monitoring dashboard. Metrics like requests serviced, latency, concurrency, memory consumption, and CPU consumption. The fun part with all of this is combining it together to pull out information. We can graph multiple metrics on the same graph to track what is happening at any given time. We can also take those graphs and make a dashboard out of them for quick reference. And all of this is offered without setting up a dedicated monitoring server and software like teams traditionally would with something like Prometheus or Grafana.

We can create custom, application-specific metrics by using metric filters on log groups like what I mentioned in the last section. These custom metrics can be a bit tricky at first because they use the log filter syntax to parse for correct logs and when they are found a configured "amount" is added to that custom metric. I suggest testing the filter syntax for a metric filter first in CloudWatch Insights before committing to it as the source of truth for a custom metric. These can also be tricky since they rely on log messages to be present in order to function correctly, so I recommend writing tests for specific log messages to prevent someone from changing a message that might seem inconsequential but is necessary for a metric.

That is about all for metrics. It is helpful having some sort of logging and monitoring experience before working with CloudWatch's metrics since the concept is fairly universal, but since metrics are fairly simple to grasp, do not worry if you have not.

### Alarms

Now that we have covered metrics we can discuss CloudWatch Alarms. Alarms are continuously monitored by AWS and based on a single metric or a combination of metrics. The basic idea with alarms is that after a certain threshold or criteria is met, an alarm is triggered, which can subsequently trigger other actions.

Setting up alarms means defining what we want to monitor. An alarm can be as simple as checking that a Lambda's execution duration stays under 500 milliseconds or as complicated as a mathematical formula for determining the state of an entire system. The single metric alarms are fairly straightforward, using metric math does get a little complicated depending on the end goal. Find current AWS documentation for using metric math though since they do still add new functionality to it.

After an alarm is set up we can define what happens when metric data stays below the configured threshold over a period of time and when it stays above the threshold for a given period of time. We also need to tell alarms how to interpret missing data, but that piece is not near as difficult and is highly dependent on the alarm and its action. Whenever the metric crosses above or below the threshold we can trigger some sort of action like autoscaling (not necessary for serverless architectures) or sending an event through SNS (there is a chapter later on about SNS). Does that last action ring any bells?

I mentioned this integration in the Lambda chapter's Integrations section, but integrating CloudWatch Alarms with SNS and Lambda opens up a world of options. The Lambda that subscribes to events can read the event, know what alarm was triggered, then do whatever we want it to. That is the beautiful thing about Lambda. It is the glue that holds together AWS services. We can tell the Lambda to send us an email, a Slack message, an incident management ping, or anything else that could fit into an alarm response workflow.

With that, we have the basics for logging and monitoring down. We know where logs go, where metrics go, how to create custom metrics, how to create alarms, and what to do with those alarms. There is one final related piece to all of this though.

### X-Ray

AWS X-Ray is a standalone AWS service but it ties in well with the CloudWatch logging and monitoring discussion. X-Ray is a tracing solution, which comes in handy the larger a system gets. As more and more services are added to an application, tracing a request through the system can become tricky. Tracing services (AWS is not the only player in this field) is a common debugging tool used to follow a request through multiple services showing the inputs, outputs, and timing along the way.

Most services that I have discussed so far in this guide can be set up to integrate with X-Ray with the flip of a switch. Under the hood, X-Ray adds a header that is passed from service to service and reported back to X-Ray as requests come and go. Each service that is configured with X-Ray and reports back is shown as a "segment". Those segments are pieced together to form a final "trace". A trace is mostly what we are after because it shows the entire picture of which services handled a request, how long they took, and the result.

Other HTTP libraries can be "patched" to include the X-Ray tracing header. Some of those libraries include JavaScript's `http` and `axios` and Python's `requests`. The benefit of patching those libraries is seeing the full picture end to end. Since most applications nowadays use third-party APIs, X-Ray needs to patch the libraries in order to create segments for the non-AWS services used.

After all of the segments are stitched together to form traces, X-Ray combines the traces for various requests together to form a "service graph". The service graph is an awesome way to visualize a system and see which components interact. Not only is the service graph fun to look at, but it is also super helpful when debugging latency bottlenecks.

### Integrations

In addition to the CloudWatch Alarms integration with SNS and Lambda, I wanted to highlight a quick few other integrations including one that I have used and had success with. I mentioned subscription filters in the Logs section, and the reason was for easy integration with Kinesis. I have brought up Kinesis before in this guide and a chapter on it is coming up, but for now, we can think of it as a pipeline where we can take in data, transform it to our liking, and deliver it to its final destination like S3. We can configure a subscription filter to integrate directly with Kinesis and push logs in batches as they are written. This allows us to push them to S3 for long-term storage that is queryable by Athena, while our log group can be configured with a short retention period to reduce cost.

Subscription filters can also be directly integrated with a Lambda function. While I have not personally used this integration before, I believe it is important to point out some of the possibilities. I imagine that this integration would be helpful for shortcutting an alarm. Instead of creating a metric filter, custom metric, alarm, and an SNS topic, we could simply subscribe a Lambda directly to the logs defined by a filter pattern and react.

## Intermission - Exclude from blog post(s), include in full book

This is the end of the in-depth chapters. Thank you for hanging in since they were long chapters. If you have come this far you should now know what IAM, VPC, Lambda, API Gateway, DynamoDB, S3, and CloudWatch are at a high level. With those seven services, you should be able to get pretty far in the AWS world. Everything else should be simpler to learn and understand. The following chapters should be shorter in length but still build on what was discussed in the earlier chapters. The shorter chapters will mostly not contain separate sections since they are less versatile, more purpose built services rather than platforms.

## CloudFront

This first shorter chapter is about Amazon CloudFront. CloudFront is a Content Delivery Network (CDN) which is a fancy way of saying it is a file server. There are tons of different CDNs available out there, but CloudFront makes integrating with S3 super easy and it has extremely nice built-in features. The way CloudFront (and CDNs in general) work is by pulling static files (images, HTML, etc.) from a specified origin to be distributed with less latency to end-users. Distributing those files amongst data centers that are geographically separated allows a user to download files from a location closer to them instead of from a single file server that may be far away resulting in higher latency (the name of the game is almost always to reduce latency). Those files will be cached near users which also helps speed up delivery.

The way CloudFront works with S3 is more or less what I outlined above. We create something in CloudFront called a "distribution," which pulls files from a specific S3 bucket and serves them to users. The integration is about that simple. If an S3 bucket hosted a website, putting a CloudFront distribution in front of the S3 bucket would not change anything except speed up time to delivery for users, the paths are a one-to-one mapping unless otherwise configured.

CloudFront first and foremost brags about its security. Just turning on CloudFront instead of using an S3 bucket for pulling files brings all the benefits that Amazon has worked so hard to create. There are other add-ons that we can enable or integrate to create extra barriers for security's sake, but vanilla CloudFront is a great starting point. Those other services and integrations are services like AWS WAF (Web Application Firewall) and AWS Shield, but keep in mind that the security services are not free.

CloudFront is also highly available if configured correctly using something called origin groups. The CloudFront and single S3 bucket approach that I have already talked about is great, but there is a chance that the region can go down where our single-origin S3 bucket lives. If that happens, our users would not be able to retrieve files. Origin groups can provide higher availability by setting a failover origin. If the primary origin or S3 bucket is unavailable, CloudFront will automatically attempt to retrieve files from the failover origin or S3 bucket. As I have already mentioned, we can configure S3 to replicate data across regions, so the CloudFront failover origin can be set up as a cross-region replicated S3 bucket for our service's continuity.

I have already hinted at CloudFront being quick and performant, but there are a few specific pieces we benefit from that I wanted to point out. First and foremost is caching. Data transfer costs money. Not a ton, but it does add up depending on the level of traffic being served. Whenever a file is transferred out of S3 to CloudFront, CloudFront caches that file which means we do not need to transfer any more data out of S3 as long as the file is cached. While there is still cost, data transfer out of CloudFront is cheaper and quicker than S3.

Edge locations are a huge advantage of CloudFront, and one of the main reasons anyone uses a CDN at all. Amazon uses CloudFront to serve their static content, so serving files quickly with their amount of traffic is no easy feat. Thanks to the numerous edge locations (data centers) all over the world that we can take advantage of, we can serve our files (websites, photos, etc.) quicker to users because servers are closer to our users. Low latency is one of the top factors for website users' satisfaction, so we have the chance to make users happier.

The last performance-based feature that I wanted to highlight is compression. CloudFront will compress objects that users are requesting to view if the user's browser (or other application) supports compression. A compressed file means a smaller payload, which translates to saved bandwidth and money. If there is less of a file to transfer across the internet, that file will naturally finish downloading faster. Since we pay CloudFront transfer fees by the gigabyte, we also pay less since we are transferring less data.

CloudFront is a huge benefit of using AWS services because it is super easy to integrate, gives added security out of the box, and allows for easy configuration to reduce latency. Whenever I have an S3 bucket hosting a website or serving files, I almost always put a CloudFront distribution in front of it. As a rule of thumb, if an end-user uses a file in an S3 bucket, I try to serve it to them through CloudFront. With the easy integration, there is almost no reason not to use S3 and CloudFront together for file sharing and website hosting.

## Route 53

Amazon's Domain Name System (DNS) is called Route 53. Route 53 integrates easily with other AWS services, is highly available, scalable, and has some interesting methods of routing traffic. I have used Route 53 in the past and enjoyed it, but it is worth pointing out that using another DNS (like the one where a domain name is purchased) does not void anyone of using custom domains on AWS. Using an AWS service over a third-party service always comes with its own set of benefits though.

One last note before I dive a little deeper into Route 53. This service is not really serverless in the way that I have defined it. Pricing is not necessarily per request, but by the very nature of DNS, I am not entirely sure if that is possible. That being said, there is no other DNS that I know of offering a "per-request" style of pricing. Either way, I feel like the flexibility and ease of integration that Route 53 has with other AWS services that I have brought up in this guide makes it useful enough to include.

The first and largest benefit that comes with using Route 53 is availability. AWS offers a 100% available SLA, which is just wild. I have never heard of another service in the world that offers 100% availability. Their strategy for providing something so ludicrous is not a secret and published in online documentation, which I suggest finding and reading to understand the power of this system that we can tap into. Again, AWS flexes their huge global presence to make an extremely available and scalable service.

A fundamental concept of Route 53 is the "hosted zone", which is the same idea as a normal DNS zone. A hosted zone needs to be created in Route 53 before DNS records can be created. There are two types of hosted zones: public and private. Private hosted zones are used for inter-VPC records, so my guess is that you will not use them often unless that type of networking is needed in your infrastructure. Public hosted zones are what will be exposed to the internet and tell users where to find the resources they are querying for.

Another fundamental concept of Route 53 is the health check. This might not sound like something that relates to normal DNS, which is correct. There are complex routing policies offered through Route 53 that rely on health checks for failovers, and I will discuss those more complex routing policies right after health checks. This concept is what it sounds like though. Route 53 checks that the target it is sending traffic to is up and ready to receive that traffic.

There are three types of health checks: normal health check, single resource health check, a combination of health checks on various resources, and a health check based on a CloudWatch Alarm. The single resource health check is a simple and standard health check that reaches out to a resource, makes sure that the resource responds, and signifies whether or not the resource is up and running.

The combination of health checks is similar to CloudWatch Alarm math where we can take the status of multiple health checks on resources and combine them to determine a final status of a system. The combination health checks work best for distributed systems that need a few critical pieces working in order for the whole system to work properly.

The final health check style should seem familiar after going through the CloudWatch chapter. The status of a CloudWatch Alarm is what signifies the health of the target. This can also be a combination of different alarms or based on a resource's load to determine whether a resource can handle more load. Since a CloudWatch Alarm is used, the possibilities are almost endless, and I suggest reading what I have previously discussed about CloudWatch Alarms to understand what can go into this.

Lastly, I wanted to bring up the various routing policies that Route 53 exposes to us. The types of routing as of this writing are simple, failover, multivalue answer, weighted, geolocation-based, latency-based, and geoproximity. We do not necessarily need to stick to a single routing policy though since AWS lets us combine them; however, I have personally never used a combinatorial approach and I imagine the use case would be fairly complex. The two routing policies that I specifically want to discuss are simple routing and latency-based routing. These are the two that I have used in projects before and find most useful. Each routing policy has its own advantages depending on users and application use case though, so they are all worth knowing about.

Simple routing is a normal DNS record. This is more of a standard single domain name to single resource mapping using record types like `A` and `CNAME` records. I have used this type of policy multiple times over because it is familiar and simple to set up. There is not much more to say about this, and if this information seems foreign to you, I would suggest looking into what DNS is before moving on.

Latency-based routing adds a bit of Route 53 flavor into the mix of a standard DNS. Instead of a single domain name mapping to a single resource, we can use the same domain name for multiple resources and dynamically point the user towards the resource that would provide them with the lowest latency. To me, this is an awesome feature of Route 53 that allows us to create dynamic and distributed applications. The situation I have used this in was a globally diverse application. There were regional deployments of API Gateways with Lambda integrations and DynamoDB Global Tables replicating to those regions. The DNS was handled by a latency-based routing policy that routed the user to the replica of the application closest to them instead of using different domain names for each region.

I have used Route 53 as the DNS service for both API Gateway and CloudFront distributions hosting websites with great success. The way that AWS allows us to replicate resources across regions and point users towards the closest set to them is an instant latency decrease. The services all flow together and make building distributed applications much easier and quicker, which is my main goal.


## SNS

When a system gets more and more complex, processing times can start to grow out of control. No matter how many tweaks or optimizations we make to API endpoints' handlers, sometimes it is best to simply move some of that logic out of the critical path. The end goal of creating an asynchronous workflow like this is the limit the amount of latency that a user experiences while finishing processing a request in the background. In traditional systems, this could mean creating a queuing service, posting messages to the queue, and consuming those messages in the background. With AWS we can accomplish all of this with the use of Simple Notification Service (SNS).

I will be honest, I did not realize the usefulness of SNS until fairly late into my AWS journey. Reducing latency can be difficult. I have found that moving logic out of the user's interaction and into the background is sometimes easier and more beneficial than trying to optimize code as much as possible. SNS is super helpful in creating these types of asynchronous workflows especially because it integrates so easily with Lambda. I brought up event-driven architectures in the Lambda chapter, and SNS is what drives those events throughout a system.

There are a few fundamental concepts revolving around SNS that need to be understood before I go into my uses cases. SNS is simply an event emitter and each "instance" of SNS is called a "topic". We can make an API call to publish an event to a topic. The topic then pushes that event out to all of its "subscriptions" based on the "filter policies" applied to them. Often times we refer to a system like this as a distributed publish-subscribe (pub/sub) messaging system, where Apache Kafka is the incumbent software.

A topic is what would be considered an instance of a queuing system. With SNS being serverless, we can choose to create as many topics as we would like, but I personally like to only create one per workload or API. This keeps everything centrally located and less confusing. I separate out my processing logic using filter policies on my subscriptions.

Subscribing to an SNS topic is fairly straightforward. We can subscribe a number of endpoint types to a topic including HTTPS, email, SMS, SQS (more on this later), Kinesis (again, more on this later), and Lambda. If the subscriber is external (HTTPS, email, SMS), then we subscribe to the endpoint, email address, or phone number. If the subscriber is another AWS service, then we can subscribe to the service directly using its ARN. Once the subscription has been established, any events published to the topic will be pushed to all of the subscriptions.

After creating a topic and while starting to create my subscriptions, I like to add filter policies to my subscriptions to maintain more granular control over what events my subscribing Lambdas handle. Filter policies are what allow me to use a single topic for multiple types of events. The only trick with using filter policies is that the same keys and values used in the filter policy need to be included in the "`Publish`" API request as the "message attributes". This is the only way that SNS will know how to deliver those events accurately.

The way I like to structure my projects for asynchronous processing now is a standard API Gateway and Lambda at the front serving the user in the critical path, an SNS topic for the API Gateway's Lambda to publish events, and a Lambda with a subscription to that topic to consume those events on the backend. Normally, emitting an event in the critical path takes less time than the extra processing would especially if the extra processing includes API calls. I also use the AWS SDK to handle constructing the `Publish` API payload in the API Gateway's Lambda. Remember that the `Publish` API call is where I need to use the same message attribute key and value pairs that I used in the filter policy. If events are not being properly delivered, the message attributes may have an inconsistency with the filter policy.

Besides using SNS for offloading logic from an API endpoint, we have already discussed a useful integration a couple of times which is CloudWatch Alarms. Now that we have a better understanding of exactly what SNS is the integration with CloudWatch Alarms is hopefully a little clearer. An Alarm triggers which publishes an event to an SNS topic which pushes the event to its subscriptions. I have personally used email and Lambda subscriptions with this integration and had great success.

While an API endpoint most likely needs its own custom logic, there is the chance that all of the work can be done asynchronously. An example of this might be order processing. Instead of integrating the API Gateway with a Lambda which would only publish an event, we can skip the Lambda and integrate API Gateway directly with SNS. This is also something I brought up in the API Gateway Proxy Integrations section of the API Gateway chapter. While the specific circumstances for which using this integration effectively might be rare, it can save some extra latency for our users whenever we can implement it.

One of the subscription integrations that I brought up earlier was AWS SQS which stands for Simple Queue Service. Wait, I thought SNS was a replacement for setting up a traditional queuing service? Kind of. SNS and SQS when used together would really be the alternative for a more traditional piece of software like Kafka. SNS works perfectly fine but SQS adds more durability into the mix. However, I will discuss my thoughts and use cases for SQS in the following chapter.

Overall, SNS offers huge benefits and easy integrations. Since I rediscovered how important it can be, I use SNS in tons of my workloads. Being able to publish events to be handled in the background frees up resources and time to ensure a smoother and snappier user experience for my APIs. The pricing also fits in nicely with the serverless models that I have already talked about. We pay for SNS by request or event, so our costs only scale as we utilize the service more. The goal, as with all applications, is that our usage increases in step with paying users.


## SQS

As I mentioned in the last chapter, Amazon SQS is a more durable message queuing service than SNS. While SNS attempts to emit events as they are published, SQS will hold onto events until an event has successfully been acted upon. Even though I compare SNS and SQS together, know that they are different services with different use cases. SNS pushes out events and SQS holds onto events for a given amount of time or until they are retrieved by some other service.

I have had success using SNS to feed events into SQS with a Lambda consuming the events in SQS. The benefit is that events emitted through SNS are held onto, but there is one caveat here. While SNS pushes events to subscribers, SQS is more of a container that requires polling. Each time that we poll SQS, there is a charge associated with the API call because we are using the service. Depending on the polling configuration, this can add up to extra charges even if events are not actively flowing through SQS. While SQS in and of itself is serverless, the usage model is not based solely on events but also on API calls for retrieving events.

My suggestion for using SQS is to make sure that there is a steady stream of events to consume otherwise the low scale might not be a justifiable use case. SQS is great at offering a buffer for handling events, so if the amount of events does not swamp a Lambda function, then it might be better to stick with only SNS. The threshold of when to add in SQS is a case-by-case decision, but it is worth a bit of caution for the extra cost even if it is small.

I have talked about polling for SQS already, and I want to quickly mention that with a Lambda integration, that polling is handled for us. This is yet another benefit of Lambda and the native integrations offered through other AWS services. I have personally only used Lambda with SQS because the integration provides long polling out of the box and it is super easy to set up. Other services can be used with SQS, but since it does not push events to subscribers, queue consumers might need to set up polling on their own using the AWS SDK or related APIs.

There are two caveats that I have personally come across while developing with SQS and Lambda that I want to point out and discuss. The first is how Lambda deletes events from a queue. The second is around how SQS delivers duplicate events.

When a Lambda function is integrated with SQS it long polls the queue until it receives a batch of events or records. Once our code successfully acts upon those records, we see the Lambda return and finish execution. Behind the curtains though, the integration knows that it needs to delete events from SQS before actually ending its execution. If our code throws an error, the event is never deleted and remains in the queue for another attempt at processing. I learned this lesson the hard way as I wondered why the events in my queue starting stacking up and were never consumed. I knew that the Lambda was running, but the problem was that my code kept erroring out causing the events to remain in the queue.

I have two suggestions for creating a more robust integration between SQS and Lambda. The first is to properly test for and log unexpected errors. I have not had good experiences simply catching and passing all errors, and I will explain why with the second suggestion. Logging the errors is important so we can see what exactly caused the problem then fix it. My second suggestion is to limit the number of times SQS will return an event when polled and move the presumably "bad" event to a dead-letter queue (DLQ) after the number of retries is reached. If all errors are simply caught and passed, then an event would never make it to the DLQ for further investigation.

A dead-letter queue is a queue where events go that could not be successfully processed in their original queue. I like to set up something like three maximum retries on events before they are forwarded to the DLQ. For the DLQ, we can set a CloudWatch Alarm that notifies us whenever we have a bad event end up there, which should prompt further investigation. Maybe the event was malformed, or maybe there was a bug in our code. Either way, this saves money by not having a Lambda function continually fail to act on a specific event.

The second caveat that I have encountered while working with this integration is that SQS does not guarantee only-once delivery. That means that two instances of the same Lambda function can pick up duplicate copies of the same event. There are configuration settings to help remediate this, but I have found better success simply writing idempotent Lambda functions. Writing idempotent Lambdas is probably a better way to go about coding anyway. Just keep this in mind if you ever find two duplicate entries in a database and wonder what happened.

SQS has its benefits, but I see those benefits more so at scale when a heavy spike of traffic needs to be properly absorbed. My personal preference when large scale is not present is to emit events to Lambda through SNS only. My reasoning is because it reduces the potential cost of polling an empty SQS queue. For more durability, I attach a DLQ to the Lambda that is handling the SNS events. The same idea for DLQs applies with Lambda, and we can attach DLQs to any asynchronously invoked Lambda. If the Lambda fails to successfully process an event, it falls into the DLQ, which I can investigate later.

## Kinesis

Another service facilitating asynchronous processing in AWS is Kinesis, which I have mentioned a few times already in this guide in the context of data analytics with S3 and CloudWatch. If you have already tried looking into Kinesis, you might have noticed that there are a few "subservices" underneath the greater umbrella. Kinesis itself is more of a family of services that help in collecting, processing, and analyzing data in real-time. Whenever I have mentioned "Kinesis", I specifically mean Kinesis Data Firehose, and the same rule applies in this section. Data Firehose is a serverless flavor of Kinesis where we pay by data ingested only, while Data Streams is a more managed flavor where we pay for provisioning by the hour. There are two other services as of this writing which are Video Streams and Data Analytics, but I have not used either service too extensively as of yet.

Any flavor of Kinesis (but specifically Data Firehose) is a queue similar to SQS but much more suited to data analytics applications because it operates near real-time. Kinesis can move tons of records at a time while also allowing for custom processing using integrated Lambda functions. The input to a Kinesis instance is logs and the output are those logs but transformed and shipped to the configured destination.

Kinesis is intended to ship data like logs from services in a distributed system to a centralized location, but it all starts with records produced by our system. Data can be pushed in using AWS integrations like CloudWatch or directly using HTTP PUTs on Kinesis's API which can be accomplished using the AWS SDK. I have also taken this a step further by creating an API Gateway proxy integration to Kinesis so that my services could PUT records to the API Gateway using an HTTP library instead of needing the SDK. This is just one more example of all the ways we can use the building blocks I have mentioned so far in this guide to build something custom for ourselves.

After records enter Kinesis, we can configure something called a transformer, which is a Lambda function that contains custom logic to transform records before they end up in their final destination. The Lambda function is invoked with a batch of records that have been buffered by Kinesis. Those records could come in compressed depending on how Kinesis is configured, and they will also be `base64` encoded. Normally my Lambda code involves iterating through the records, decoding them to ASCII, performing my custom transformations, encoding back to `base64`, then returning. Transforming Kinesis records normally involves normalizing, removing, scrubbing, or injecting data. That piece is custom so there is not much specific advice that I can provide on that front; however, returning the transformed data in the proper format to Kinesis has some trip-ups.

First off we need to return data to Kinesis encoded as `base64`. I have had my fair share of simply returning a transformed plain text record that resulted in me wondering where it went.

The more confusing part of returning records to Kinesis is the returned object. It is not enough for us to return a `base64` encoded string because Kinesis will not know what the original record was, where it was going, or if the transformation was successful. Each record is given to the Lambda as an object and needs to be returned as an object. The values of the returned object are the record's ID (this is part of the input), the record's status (whether the transformation was successful), and the record itself.

The ID is included in the object from the start and needs to be kept track of. The transformed record data is up to us to create and encode. The record's status is going to be one of three values (at least as of the time of my writing) which signify whether the transformation was successful, unsuccessful, or the record should be dropped/deleted. Unsuccessfully process records go to a specially configured location for this type of log, and we can also set up a CloudWatch Alarm to monitor failed processing attempts for us. Dropped records are dismissed. And successful records move on to their final destination.

After transformation, data is pushed to its final destination. Kinesis has integrations with data analytics and monitoring related services like S3, ElasticSearch, Datadog, and many others. This final piece of configuration contains nuances for each integration, so it is best to read current documentation whenever the integration is set up. I have personally had success pushing to the three destinations that I listed, but I know that there are many others that I am sure work out equally as well.

Since data analytics is more of a necessity for larger-scale applications, I have not used Kinesis much in the context of my personal projects. I have had great success using it for enterprise-level applications though. Data flows easily and near real-time from multiple scattered resources and even different AWS accounts into a centralized destination for viewing, debugging, and analytics purposes. Log consolidation is extremely helpful from a developer and business standpoint.


## Developer Tools Family

CodeBuild, CodeDeploy, and CodePipeline are three services that do not operate our application but can be crucial in developing it. All are under the AWS Developer Tools family which helps implement Continuous Integration and Continuous Deployment (CI/CD) workflows. I felt like these services deserved at least a mention since I have used them so extensively, and I believe that most developers and companies should try to implement CI/CD. There are loads of services out there available to help implement CI/CD but I prefer these because of their advantage of native AWS integrations.

CodeBuild is what it sounds like: a build server. A CodeBuild instance is configured to trigger a build based on a git repository either from webhooks, manually, or through an API call. Once a build starts, CodeBuild references something called a `buildspec.yml` in the source code to run. The `buildspec` is a series of shell commands to install, lint, test, package, and whatever else is necessary to create our deployment package. Each build is run on a clean Linux image (or any other image configured), so we can perform any action we can on development machines to create our output. Normally, my outputs are deployment packages for Lambda that I push to S3, but I have also built and pushed Docker images to ECR and performed other actions not necessarily related to deploying code.

In the same way, CodeDeploy is also what it sounds like: a service to deploy code. The difference between deploying with CodeDeploy and CloudFormation is that we can get fancier than a simple all-at-once deployment and we can enable automatic rollbacks. While CodeDeploy can deploy code like CloudFormation with an all-at-once strategy, we can also enable blue/green deployments using canary and linear approaches, which are much safer than all-at-once deployments. Canary deployments shift traffic in two increments with the first being a smaller portion of traffic before shifting the rest of the traffic after a deployment is deemed safe. Linear deployments shift traffic in multiple equal increments over a configured amount of time. Both canary and linear deployments can automatically roll back deployments at any point based on CloudWatch Alarms that would trigger if something fatal to our service occurs.

Lastly, CodePipeline is a CI/CD workflow orchestrator. The same way AWS Step Functions orchestrates multiple services for serverless workflows so does CodePipeline for CI/CD workflows. I have personally integrated CodePipeline before with S3, CodeBuild, CodeDeploy, Lambda, and CloudFormation among others. Notice the Lambda integration? Again, Lambda is the glue between AWS services, and once a service can use Lambda, anything is possible. CodePipeline always needs a "source stage," which is where it either pulls in a deployment package from S3, a container from ECR, or is triggered by a git repository webhook from something like GitHub. After the source stage, we can combine different actions with inputs and outputs into stages that build, test, and deploy code. The ideal situation is to continuously deploy code, but we can also continuously deliver code and wait for manual approval before deploying.

One implementation idea could be as follows. CodePipeline is integrated with a GitHub webhook to pull in code whenever it is merged into a mainline branch. CodePipeline could then feed the source code into CodeBuild to build the actual deployment package and push it to a predetermined S3 bucket. The location and version of the new S3 object could be output by CodeBuild to be fed into a deployment stage consisting of a CloudFormation action. The CloudFormation action would then perform an all-at-once deployment of the new Lambda code. This effectively implements a CI/CD workflow where source code is automatically deployed without a developer needing to perform tasks outside of merging a pull request.


## Serverless Containers

I am hesitant about bringing up container-related services for this guide, which are Amazon Elastic Container Registry (ECR), Elastic Container Service (ECS), and Fargate. While they are marketed as serverless, they do not exactly fit my bill for being serverless since there is still much to be managed by us. Not only is management more involved, but there is a constant cost associated with compute time being billed by the hour since the underlying compute infrastructure is constantly provisioned instead of being charged by requests handled. All that being said, these services are still a good step in the serverless direction compared to the infrastructure that many teams and companies still run.

I have mentioned ECR a couple of times so far in this guide. The same way Lambda deployment packages are to S3, so are container images to ECR. ECR is a competitor to Docker Hub, and AWS has recently been trying to take them on by providing more lenient download quotas and hosting public images. Either way, we can create public and private image repositories that will hold our images for us. Naturally, AWS services are well integrated with ECR to make deploying images to Fargate as easy as deploying code to Lambda. That is about it for a high-level overview of ECR. I treat it as a simple resting place for images.

Now we can learn how to run containers starting with ECS. ECS is a control plane much like Kubernetes is, and I would not be surprised if some Amazon hacked version of Kubernetes lives under the hood. Before we can even start launching containers, we even create something called an "ECS cluster". ECS is responsible for determining which containers to create, how many to create, and then maintaining the desired state.

There are two fundamental concepts in ECS for defining containers and how they should run, which will seem familiar if you have Kubernetes experience. We use "services" and "tasks" in ECS to define how containers should be orchestrated. A task is a set of containers that should be run together. Most of the time a task only has one main container with business logic in it, but we can deploy auxiliary containers sometimes called sidecars. A service defines some information about how to run and maintain a certain number of tasks within the ECS cluster.

Most people online including me, normally refer to this whole suite of services as simply "Fargate," but Fargate itself is a small piece of deploying containers using ECS. Fargate is the compute platform onto which ECS deploys containers. The alternative to letting ECS deploy containers on Fargate is deploying them on EC2 instances, which requires much more operational overhead and is not serverless. I like to think of Fargate more or less as a preconfigured set of EC2 instances that are ready to be used specifically with ECS.

In my opinion, Fargate is only considered serverless because the alternative is obviously not serverless. If we chose to use EC2 instances with ECS, we would need to create AMIs (VM images) that allow ECS to run containers and manage the amount of EC2 instances available to an ECS cluster at any given time. While it is not impossible to set up EC2 instances to run with ECS, it does introduce much more complexity than simply using a managed version in Fargate.

Where Fargate is not serverless in my opinion is billing and built-in administrative features like autoscaling. Fargate makes running containers in the cloud easier, but the amount of instances needed at any given point in time needs to be determined by us. A common strategy is setting up CloudWatch alarms and Autoscaling Groups to scale the number of tasks up whenever they pass the alarm's threshold and back down whenever the alarm quiets down. It works just fine, but it means more management and overhead for us. Compared to Lambda where thinking of autoscaling is almost a non-factor, this is just another downside to ECS and Fargate.

Fargate uses EC2 instances under the hood, which have a time-based charge associated with them. Depending on the amount of memory and CPU power we want available to our containers, we pay more and more per hour to host those containers. No matter what though, we are paying per hour, not per request. Even with a new project trying to get off of the ground, AWS would rightly charge us no matter how much the instances are being utilized. It also means development and staging environments incur a charge when they are up whether or not they are being used.

Use cases for applications vary and using constantly provisioned compute instances does come with benefits sometimes. Maybe traffic patterns are well known and consistent, which would make scaling less of a factor. Maybe the programs being run are more resource-intensive and heavier, which would make Lambda's timeouts and lean nature unappealing. Maybe developing with containers is best for a team, which makes Fargate a step in the right direction compared to EC2. (Although, we can run containers using Lambda, which is very similar to running them in Fargate.) While I still personally prefer using Lambda, Fargate might be a step in the right direction that a team needs to incrementally adopt serverless technology.

While I do shed some sense of a negative light on Fargate, I have used it for production workloads with success, and I do not dislike Fargate as a service. At the end of the day, a development team will probably choose a set of technologies that is most familiar to them, and containers, ECS, and Fargate are closer to prior technology stacks than Lambda will be. This is the reason I decided to include these services in my guide. If all of this is new to you though, then my suggestion is to stick with Lambda as a compute platform. The lower learning overhead makes it easier to get up to speed and Lambda is a great platform to build APIs for modern web applications.


## My Personal AWS Platform as a Service - TODO

My method of creating on AWS has become creating similar groupings of infrastructure for common tasks based on experience of what works well together, and I hope that by now, you will be able to start piecing something together in your mind. CloudFormation templates are a great start, and I have hundreds of lines worth of CloudFormation templates on GitHub that I have reused multiple times over as both a solution and a starting point. I have created a Platform as a Service for myself in a way by abstracting those common templates out that I can recreate for any purpose.

Lately, I have been focusing on building a PaaS of sorts for myself using the CDK. Since the CDK outputs CloudFormation templates, I more or less already had a starting point for myself by referencing all of the static CloudFormation templates that I had previously created for the same reason. My personal preference is to default to the CDK from now on although I have seen hesitancy in others online because the CDK is not declarative like static templates are. Either way, I suggest learning and using AWS with building your own platform as a goal. While most of infrastructure and operations can be boiled down to a science, I would rather treat it like art and make it my own.
